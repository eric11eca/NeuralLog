{
 "cells": [
  {
   "source": [
    "# UdepLog Neural-Logical Inference System"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tree import Tree\n",
    "from nltk.draw import TreeWidget\n",
    "from nltk.draw.util import CanvasFrame\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def jupyter_draw_nltk_tree(tree):\n",
    "    cf = CanvasFrame()\n",
    "    tc = TreeWidget(cf.canvas(), tree)\n",
    "    tc['node_font'] = 'arial 14 bold'\n",
    "    tc['leaf_font'] = 'arial 14'\n",
    "    tc['node_color'] = '#005990'\n",
    "    tc['leaf_color'] = '#3F8F57'\n",
    "    tc['line_color'] = '#175252'\n",
    "    cf.add_widget(tc, 20, 20)\n",
    "    os.system('rm -rf ../data/tree.png')\n",
    "    os.system('rm -rf ../data/tree.ps')\n",
    "    cf.print_to_file('../data/tree.ps')\n",
    "    cf.destroy()\n",
    "    os.system('convert ../data/tree.ps ../data/tree.png')\n",
    "    display(Image(filename='../data/tree.png'))"
   ]
  },
  {
   "source": [
    "## 2. BERT Model for Pharaphrase"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at textattack/roberta-base-MRPC were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Load Alignment Model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "roberta_MRPC = \"textattack/roberta-base-MRPC\"\n",
    "bert_MRPC = \"bert-base-cased-finetuned-mrpc\"\n",
    "\n",
    "paraphraseTokenizer = AutoTokenizer.from_pretrained(roberta_MRPC)  \n",
    "paraphraseModel = AutoModelForSequenceClassification.from_pretrained(roberta_MRPC)\n",
    "paraphraseModel.to('cuda')\n",
    "print(\"Load Alignment Model\")"
   ]
  },
  {
   "source": [
    "## 3. UD Parser and RoBERTa Semantic Similarity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-03-20 00:18:52 INFO: Loading these models for language: en (English):\n",
      "========================================\n",
      "| Processor | Package                  |\n",
      "----------------------------------------\n",
      "| tokenize  | ../model/e...ize/gum.pt  |\n",
      "| pos       | ../model/en/pos/ewt.pt   |\n",
      "| lemma     | ../model/en/lemma/gum.pt |\n",
      "| depparse  | ../model/e...rse/gum.pt  |\n",
      "========================================\n",
      "\n",
      "2021-03-20 00:18:52 INFO: Use device: gpu\n",
      "2021-03-20 00:18:52 INFO: Loading: tokenize\n",
      "2021-03-20 00:18:53 INFO: Loading: pos\n",
      "2021-03-20 00:18:53 INFO: Loading: lemma\n",
      "2021-03-20 00:18:53 INFO: Loading: depparse\n",
      "2021-03-20 00:18:54 INFO: Done loading processors!\n",
      "2021-03-20 00:18:54 INFO: Loading these models for language: en (English):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | ../model/e...ize/gum.pt |\n",
      "=======================================\n",
      "\n",
      "2021-03-20 00:18:54 INFO: Use device: cpu\n",
      "2021-03-20 00:18:54 INFO: Loading: tokenize\n",
      "2021-03-20 00:18:54 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from wordnet import *\n",
    "from copy import deepcopy\n",
    "from Udep2Mono.util import det_mark, det_type\n",
    "from Udep2Mono.util import btree2list\n",
    "from Udep2Mono.dependency_parse import tokenizer\n",
    "from Udep2Mono.dependency_parse import dependency_parse\n",
    "from Udep2Mono.binarization import BinaryDependencyTree\n",
    "from Udep2Mono.polarization import PolarizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentenceTransformer = SentenceTransformer(\"roberta-large-nli-stsb-mean-tokens\")\n",
    "sentenceTransformer.to('cuda')\n",
    "\n",
    "def inference_sts(seq1s, seq2s, dist=False):\n",
    "    embeddings1 = sentenceTransformer.encode(seq1s, convert_to_tensor=True)\n",
    "    embeddings2 = sentenceTransformer.encode(seq2s, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    distance = torch.dist(embeddings1, embeddings2)\n",
    "    if dist:\n",
    "        return distance\n",
    "    return cosine_scores[0][0]"
   ]
  },
  {
   "source": [
    "## 4. Phrasal Monotonicity Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import pluralize, singularize\n",
    "from copy import copy\n",
    "import re\n",
    "import torch\n",
    "\n",
    "class PhrasalGenerator:\n",
    "    def __init__(self):\n",
    "        self.deptree = None\n",
    "        self.annotated = None\n",
    "        self.original = None\n",
    "        self.kb = {}\n",
    "        self.hypothesis = \"\"\n",
    "        self.tree_log = []\n",
    "        self.sent_log = []\n",
    "        self.stop_critarion = False\n",
    "        self.mod_at_left = [\n",
    "            \"advmod\", \"amod\", \"advmod:count\", \n",
    "            \"acl:relcl\", \"obl\", 'obl:npmod', \"det\",\n",
    "            \"obl:tmod\", \"nmod\", \"nmod:npmod\", \n",
    "            \"nmod:poss\", \"nmod:tmod\", \"obl:npmod\",\n",
    "            \"acl\", \"advcl\", \"xcomp\", \"ccomp\", \n",
    "            'compound:ptr']\n",
    "        self.mod_at_right = [\"appos\"] #\"obj\"\n",
    "        self.mod_symmetric = [\"conj\", \"compound\"]\n",
    "        self.mod_special = [\"nsubj\"]\n",
    "        self.implicative = {\n",
    "            \"watching\": 1\n",
    "        }\n",
    "        \n",
    "        '''  \n",
    "            \"cop\": self.generate_inherite, \n",
    "            \"expl\": self.generate_expl,\n",
    "            \"nummod\": self.generate_nummod,\n",
    "        '''\n",
    "\n",
    "    def deptree_generate(self, tree, annotated, original):\n",
    "        self.stop_critarion = False\n",
    "        self.tree_log = []\n",
    "        self.sent_log = []\n",
    "        self.deptree = tree.copy()\n",
    "        self.original = original  \n",
    "        self.annotated = deepcopy(annotated)\n",
    "        self.sentence = original\n",
    "        self.generate(self.deptree)\n",
    "\n",
    "    def generate(self, tree):\n",
    "        if self.stop_critarion:\n",
    "            return\n",
    "        if not tree.is_tree:\n",
    "            self.generate_default(tree)\n",
    "        else:\n",
    "            generation = self.get_generation_type(tree)\n",
    "            #print(generation, tree.val)\n",
    "            generation(tree)\n",
    "\n",
    "    def get_generation_type(self, tree):\n",
    "        if tree.val in self.mod_special:\n",
    "            return self.generate_special\n",
    "\n",
    "        disjunction = False\n",
    "        if tree.val == \"conj\":\n",
    "            disjunction |= self.search_dependency('or', tree.left)\n",
    "            disjunction |= self.search_dependency('and', tree.left)\n",
    "        \n",
    "        left_mod = tree.left.mark == \"+\"\n",
    "        left_mod = left_mod or tree.left.mark == \"=\" or disjunction\n",
    "        left_mod = left_mod and tree.val in self.mod_at_left\n",
    "\n",
    "        right_mod = tree.right.mark == \"+\" or tree.right.mark == \"=\" or disjunction \n",
    "        right_mod = right_mod and tree.val in self.mod_at_right\n",
    "\n",
    "        sym_mod = tree.val in self.mod_symmetric and tree.left.mark == \"+\" and tree.right.mark == \"+\"\n",
    "\n",
    "        if left_mod:\n",
    "            return self.left_modifier_generate\n",
    "        elif right_mod:\n",
    "            return self.right_modifier_generate\n",
    "        elif sym_mod:\n",
    "            return self.symmetric_generate\n",
    "        else:\n",
    "            return self.generate_default\n",
    "\n",
    "    def generate_special(self, tree):\n",
    "        if tree.val == \"nsubj\":\n",
    "            if tree.left.val == \"who\" and tree.right.val == \"aux\":\n",
    "                self.left_modifier_generate(tree)\n",
    "\n",
    "        self.generate(tree.left)\n",
    "        self.generate(tree.right)\n",
    "\n",
    "    def delete_cc(self, tree):\n",
    "        if tree.val == \"cc\" and tree.left.val != \"but\":\n",
    "            self.delete_modifier(tree, tree.right)\n",
    "\n",
    "        if tree.is_tree:\n",
    "            self.delete_cc(tree.left)\n",
    "            self.delete_cc(tree.right)\n",
    "\n",
    "    def delete_modifier(self, tree, modifier):\n",
    "        tree.val = modifier.val\n",
    "        tree.mark = modifier.mark\n",
    "        tree.pos = modifier.pos\n",
    "        tree.id = modifier.id\n",
    "        \n",
    "        tree.is_tree = modifier.is_tree\n",
    "        tree.is_root = modifier.is_root\n",
    "\n",
    "        tree.left = modifier.left\n",
    "        tree.right = modifier.right\n",
    "\n",
    "        self.delete_cc(tree)\n",
    "        self.save_tree()\n",
    "\n",
    "    def delete_left_modifier(self, tree):\n",
    "        #print(\"Delet: \", tree.left.val)\n",
    "        self.delete_modifier(tree, tree.right)\n",
    "\n",
    "    def delete_right_modifier(self, tree):\n",
    "        #print(\"Delet: \", tree.right.val)\n",
    "        self.delete_modifier(tree, tree.left)\n",
    "\n",
    "    def rollback(self, tree, backup):\n",
    "        tree.val = backup.val\n",
    "        tree.left = deepcopy(backup.left)\n",
    "        tree.right = deepcopy(backup.right)\n",
    "        tree.mark = backup.mark\n",
    "        tree.pos = backup.pos\n",
    "        tree.id = backup.id\n",
    "        tree.is_tree = backup.is_tree\n",
    "        tree.is_root = backup.is_root\n",
    "\n",
    "    def symmetric_generate(self, tree):\n",
    "        self.right_modifier_generate(tree)\n",
    "        self.left_modifier_generate(tree)\n",
    "        #self.delete_cc(tree)\n",
    "\n",
    "    def right_modifier_generate(self, tree):\n",
    "        left = tree.left\n",
    "        right = tree.right\n",
    "        backup = deepcopy(tree)\n",
    "\n",
    "        self.delete_right_modifier(tree)\n",
    "        self.save_tree()\n",
    "        self.rollback(tree, backup)    \n",
    "        \n",
    "        self.generate(tree.left)\n",
    "        self.generate(tree.right)\n",
    "\n",
    "    def left_modifier_generate(self, tree):\n",
    "        left = tree.left\n",
    "        right = tree.right\n",
    "        backup = deepcopy(tree)\n",
    "\n",
    "        self.delete_left_modifier(tree)\n",
    "        self.save_tree()\n",
    "        self.rollback(tree, backup)   \n",
    "\n",
    "        self.generate(tree.left)\n",
    "        self.generate(tree.right)\n",
    "    \n",
    "    def return_last_leaf(self, tree):\n",
    "        max_id = 0\n",
    "        max_id_l = 0\n",
    "        max_id_r = 0\n",
    "\n",
    "        if tree.id != None:\n",
    "            max_id = int(tree.id)\n",
    "    \n",
    "        if tree.left.is_tree:\n",
    "            max_id_l = self.return_last_leaf(tree.left)\n",
    "        else:\n",
    "            max_id_l = tree.left.id\n",
    "\n",
    "        if tree.right.is_tree:\n",
    "            max_id_r = self.return_last_leaf(tree.right)\n",
    "        else:\n",
    "            max_id_r = tree.right.id\n",
    "\n",
    "        return max(max_id, max(max_id_l, max_id_r))\n",
    "\n",
    "    def return_first_leaf(self, tree):\n",
    "        min_id = 100\n",
    "        min_id_l = 100\n",
    "        min_id_r = 100\n",
    "\n",
    "        if tree.id != None:\n",
    "            min_id = int(tree.id)\n",
    "    \n",
    "        if tree.left.is_tree:\n",
    "            min_id_l = self.return_last_leaf(tree.left)\n",
    "        else:\n",
    "            min_id_l = tree.left.id\n",
    "\n",
    "        if tree.right.is_tree:\n",
    "            min_id_r = self.return_last_leaf(tree.right)\n",
    "        else:\n",
    "            min_id_r = tree.right.id\n",
    "\n",
    "        return min(min_id, min(min_id_l, min_id_r))\n",
    "\n",
    "    def add_modifier_sent(self, tree, modifier, direct=0): \n",
    "        sentence = deepcopy(self.sentence)\n",
    "        if direct == 0:\n",
    "            last_leaf = self.return_first_leaf(tree)\n",
    "            sentence.insert(last_leaf-1, modifier)\n",
    "        elif direct == 1:\n",
    "            last_leaf = self.return_last_leaf(tree)\n",
    "            sentence.insert(last_leaf, modifier)        \n",
    "\n",
    "        self.remove_adjcent_duplicate(sentence)\n",
    "        sentence = ' '.join(sentence)\n",
    "        sentence = sentence.replace(\"-\", \" \")\n",
    "        sentence = sentence.replace(\" 's\", \"'s\")\n",
    "\n",
    "        if abs(len(sentence) - len(self.hypothesis)) < 15:\n",
    "            re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', sentence, flags = re.I)\n",
    "            sentence = sentence.strip() \n",
    "            \n",
    "            if sentence.lower() == self.hypothesis.lower():\n",
    "                self.stop_critarion = True\n",
    "                self.sent_log.append((sentence, 1.0))\n",
    "                return\n",
    "                \n",
    "            similarity = inference_sts([sentence], [self.hypothesis])\n",
    "            if similarity > 0.90:\n",
    "                self.sent_log.append((sentence, similarity))\n",
    "            if similarity > 0.97:\n",
    "                self.sent_log.append((sentence, similarity))\n",
    "                self.stop_critarion = True\n",
    "\n",
    "    def add_modifier_lexical(self, tree, modifier, head, word_id, direct=0):\n",
    "        if direct == 0:\n",
    "            generated = ' '. join([modifier, head])\n",
    "        else:\n",
    "            generated = ' '. join([head, modifier])\n",
    "        \n",
    "        sentence = deepcopy(self.sentence)\n",
    "        diff = 0\n",
    "        if word_id > len(sentence):\n",
    "            diff = word_id - len(sentence)\n",
    "\n",
    "        goal = word_id-1-diff\n",
    "        sentence[goal] = \"DEL\"\n",
    "        sentence[goal:goal] = generated.split(' ')\n",
    "\n",
    "        if abs(len(sentence) - len(self.hypothesis.split(' '))) < 7:\n",
    "            self.remove_adjcent_duplicate(sentence)\n",
    "            sentence = ' '.join(sentence)\n",
    "            sentence = sentence.replace(\"DEL \", \"\")\n",
    "            sentence = sentence.replace(\"DEL\", \"\")\n",
    "            sentence = sentence.replace(\"-\", \" \")\n",
    "            sentence = sentence.replace(\" 's\", \"'s\")\n",
    "            re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', sentence, flags = re.I)\n",
    "            sentence = sentence.strip()\n",
    "\n",
    "            if sentence.lower() == self.hypothesis.lower():\n",
    "                self.stop_critarion = True\n",
    "                self.sent_log.append((sentence, 1.0))\n",
    "                return\n",
    "            \n",
    "            similarity = inference_sts([sentence], [self.hypothesis])\n",
    "            if similarity > 0.9:\n",
    "                self.sent_log.append((sentence, similarity))\n",
    "            if similarity > 0.97:\n",
    "                self.sent_log.append((sentence, similarity))\n",
    "                self.stop_critarion = True\n",
    "\n",
    "    def generate_default(self, tree):\n",
    "        VP_rel = {\n",
    "            \"aux\":1, \n",
    "            \"obj\":1, \n",
    "            \"obl\":1, \n",
    "            \"xcomp\":1, \n",
    "            \"ccomp\":1,\n",
    "            \"aux:pass\":1, \n",
    "            \"obl:tmod\":1, \n",
    "            \"obl:npmod\":1\n",
    "        }\n",
    "\n",
    "        VP_mod = {\n",
    "            \"advcl\":1, \n",
    "            \"xcomp\":1, \n",
    "            \"ccomp\":1,\n",
    "            \"obj\":1, \n",
    "            \"advmod\":1, \n",
    "            \"obl\":1, \n",
    "            \"obl:tmod\":1,\n",
    "            \"obl:nmod\":1, \n",
    "            \"parataxis\":1, \n",
    "            \"conj\":1\n",
    "        }\n",
    "\n",
    "        NP_rel = {\n",
    "            \"amod\":1,\n",
    "            \"compound\":1,\n",
    "            \"det\":1,\n",
    "            \"mark\":1,\n",
    "            \"nmod:poss\":1,\n",
    "            \"flat\":1,\n",
    "            \"acl:relcl\":1,\n",
    "            \"acl\":1,\n",
    "            \"nmod\":1\n",
    "        }\n",
    "\n",
    "        NP_mod = {\n",
    "            \"amod\":1,\n",
    "            \"compound\":1,\n",
    "            \"det\":1,\n",
    "            \"mark\":1,\n",
    "            \"nmod:poss\":1,\n",
    "            \"flat\":1,\n",
    "        }\n",
    "\n",
    "        if tree.pos is not None:\n",
    "            if (\"NN\" in tree.pos or \"JJ\" in tree.pos) and tree.mark == \"-\":\n",
    "                for rel in [\"amod\", \"compound\", \"det\", \"mark\", \"nmod:poss\", \"flat\", \"conj\", \"nummod\"]:\n",
    "                    if rel in self.kb:\n",
    "                        for phrase in self.kb[rel]:\n",
    "                            if phrase['head'] == tree.val:\n",
    "                                self.add_modifier_lexical(tree, phrase['mod'], tree.val, tree.id)\n",
    "                for rel in [\"amod\", \"acl:relcl\", \"compound\", \"acl\", \"nmod\"]:\n",
    "                    if rel in self.kb:\n",
    "                        for phrase in self.kb[rel]:\n",
    "                            if phrase['head'] == tree.val:\n",
    "                                self.add_modifier_lexical(tree, phrase['mod'], tree.val, tree.id, 1)\n",
    "                \n",
    "            elif \"VB\" in tree.pos and tree.mark == \"-\":\n",
    "                for rel in [\"advmod\"]:\n",
    "                    if rel in self.kb:\n",
    "                        for phrase in self.kb[rel]:\n",
    "                            self.add_modifier_lexical(tree, phrase['mod'], tree.val, tree.id)\n",
    "                            self.add_modifier_lexical(tree, phrase['mod'], tree.val, tree.id, 1)\n",
    "\n",
    "        elif VP_rel.get(tree.val, 0) and tree.mark == \"-\":\n",
    "            for rel in VP_mod:\n",
    "                if rel in self.kb:\n",
    "                    for phrase in self.kb[rel]:\n",
    "                        self.add_modifier_sent(tree, phrase['mod'], direct=1)\n",
    "\n",
    "        elif NP_rel.get(tree.val, 0) and tree.mark == \"-\":\n",
    "            for rel in NP_mod:\n",
    "                if rel in self.kb:\n",
    "                    for phrase in self.kb[rel]:\n",
    "                        self.add_modifier_sent(tree, phrase['mod'], direct=0)\n",
    "        \n",
    "        if VP_rel.get(tree.val, 0) and tree.right.val == \"watching\":\n",
    "            self.save_tree(tree=tree.left)\n",
    "        if tree.is_tree:\n",
    "            self.generate(tree.left)\n",
    "            self.generate(tree.right)  \n",
    "\n",
    "    def save_tree(self, tree=None):\n",
    "        if tree is None:\n",
    "            leaves = self.deptree.sorted_leaves().popkeys()\n",
    "            tree_copy = self.deptree.copy()\n",
    "        else:\n",
    "            leaves = tree.sorted_leaves().popkeys()\n",
    "            tree_copy = tree.copy()\n",
    "        \n",
    "        sentence = ' '.join([x[0] for x in leaves])\n",
    "        sentence = sentence.replace(\"-\", \" \")\n",
    "        if sentence.lower() == self.hypothesis.lower():\n",
    "            self.tree_log = []\n",
    "            self.stop_critarion = True\n",
    "            self.tree_log.append((tree_copy, sentence, 1.0))\n",
    "            return\n",
    "        \n",
    "        similarity = inference_sts([sentence], [self.hypothesis])\n",
    "        print(sentence, similarity)\n",
    "        if similarity > 0.6:\n",
    "            self.tree_log.append((tree_copy, sentence, similarity))\n",
    "        if similarity > 0.97:\n",
    "            self.tree_log = []\n",
    "            self.tree_log.append((tree_copy, sentence, similarity))\n",
    "            self.stop_critarion = True\n",
    "    \n",
    "    def remove_adjcent_duplicate(self, string):\n",
    "        to_remove = -1\n",
    "        for i in range(len(string)-1):\n",
    "            if string[i] == string[i+1]:\n",
    "                to_remove = i\n",
    "        if to_remove > -1:\n",
    "            del string[to_remove]\n",
    "\n",
    "    def search_dependency(self, deprel, tree):\n",
    "        if tree.val == deprel:\n",
    "            return True\n",
    "        else:\n",
    "            right = tree.right\n",
    "            left = tree.left\n",
    "\n",
    "            left_found = False\n",
    "            right_found = False\n",
    "\n",
    "            if right is not None:\n",
    "                right_found = self.search_dependency(deprel, right)\n",
    "\n",
    "            if left is not None:\n",
    "                left_found = self.search_dependency(deprel, left)\n",
    "\n",
    "            return left_found or right_found\n",
    "    \n",
    "    def Diff(self, li1, li2):\n",
    "        return (list(list(set(li1)-set(li2)) + list(set(li2)-set(li1))))    \n",
    "    \n",
    "    def preprocess(self, sentence):\n",
    "        preprocessed = sentence.replace(\".\", \"\").replace(\"!\", \"\").replace(\"?\", \"\")\n",
    "        preprocessed = preprocessed.replace(\"can't\", \"can not\")\n",
    "        preprocessed = preprocessed.replace(\"couldn't\", \"could not\")\n",
    "        preprocessed = preprocessed.replace(\"don't\", \"do not\")\n",
    "        preprocessed = preprocessed.replace(\"doesn't\", \"does not\")\n",
    "        preprocessed = preprocessed.replace(\"isn't\", \"is not\")\n",
    "        preprocessed = preprocessed.replace(\"won't\", \"will not\")\n",
    "        preprocessed = preprocessed.replace(\"wasn't\", \"was not\")\n",
    "        preprocessed = preprocessed.replace(\"weren't\", \"were not\")\n",
    "        preprocessed = preprocessed.replace(\"didn't\", \"did not\")\n",
    "        preprocessed = preprocessed.replace(\"aren't\", \"are not\")\n",
    "        preprocessed = preprocessed.replace(\"it's\", \"it is\")\n",
    "        preprocessed = preprocessed.replace(\"wouldn't\", \"would not\")\n",
    "        preprocessed = preprocessed.replace(\"There's\", \"There is\")\n",
    "        return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifier_relation = {\n",
    "    \"NN\": [\"amod\", \"nmod\", \"acl:relcl\", \"fixed\", \"compound\", \"det\", \"nmod:poss\", \"conj\", \"nummod\"],\n",
    "    \"VB\": [\"advmod\", \"acl\", \"obl\", \"xcomp\", \"advcl\", \"obl:tmod\", \"parataxis\", \"obj\",\"ccomp\"]\n",
    "}\n",
    "\n",
    "def down_right(tree):\n",
    "    if(tree.right == None):\n",
    "        return tree\n",
    "    return down_right(tree.right)\n",
    "\n",
    "def down_left(tree):\n",
    "    if(tree.left == None):\n",
    "        return tree\n",
    "    return down_left(tree.left)\n",
    "\n",
    "def collect_modifiers(tree, sent_set, mod_type=\"NN\"):\n",
    "    leaves = []\n",
    "    if tree.is_tree:\n",
    "        if tree.val in [\"mark\", \"case\", \"compound\", \"flat\", \"nmod\"]:\n",
    "            leaves.append(\n",
    "                (list(tree.right.sorted_leaves().popkeys()),\n",
    "                down_right(tree.left).val)\n",
    "            )\n",
    "        if tree.val in modifier_relation[mod_type]:\n",
    "            leaves.append(\n",
    "                (list(tree.left.sorted_leaves().popkeys()),\n",
    "                down_right(tree.right).val)\n",
    "            )\n",
    "\n",
    "        for leave in leaves:\n",
    "            if len(leave) > 0 and len(leave) < 10:\n",
    "                head = leave[1]\n",
    "                modifier = ' '.join([x[0] for x in leave[0]])\n",
    "                if tree.val in sent_set:\n",
    "                    sent_set[tree.val].append({'head': head,'mod': modifier})\n",
    "                else:\n",
    "                    sent_set[tree.val] = [{'head': head,'mod': modifier}]\n",
    "        \n",
    "        collect_modifiers(tree.left, sent_set, mod_type)\n",
    "        collect_modifiers(tree.right, sent_set, mod_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n====================================\n\nInit Premise: A group of scouts are hiking through the grass\n\nHypothesis: People are walking\n{}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A group are hiking through the grass tensor(0.5427)\n",
      "A group are hiking through the grass tensor(0.5427)\n",
      "group of scouts are hiking through the grass tensor(0.4559)\n",
      "group of scouts are hiking through the grass tensor(0.4559)\n",
      "A group of scouts are hiking tensor(0.5326)\n",
      "A group of scouts are hiking tensor(0.5326)\n",
      "A group of scouts are hiking through grass tensor(0.4610)\n",
      "A group of scouts are hiking through grass tensor(0.4610)\n",
      "\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "up = [\"A group of scouts are hiking through the grass\"]\n",
    "up_h = [\"People are walking\"]\n",
    "\n",
    "annotations = []\n",
    "phrasalGenerator = PhrasalGenerator()\n",
    "pipeline = PolarizationPipeline(verbose=0)\n",
    "for i in range(len(up)):\n",
    "    premise = up[i]\n",
    "    hypothesis = up_h[i]\n",
    "    premise = phrasalGenerator.preprocess(premise)\n",
    "    hypothesis = phrasalGenerator.preprocess(hypothesis)\n",
    "\n",
    "    tokenized = tokenizer(premise).sentences[0].words\n",
    "    tokens = [tok.text for tok in tokenized]\n",
    "\n",
    "    print(\"\\n====================================\")\n",
    "    print(\"\\nInit Premise: \" + premise)\n",
    "    print(\"\\nHypothesis: \" + hypothesis)\n",
    "\n",
    "    h_parsed, replaced = dependency_parse(hypothesis, parser=\"stanza\")\n",
    "    h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "    pipeline.modify_replacement(h_tree, replaced)\n",
    "    phrases = {} \n",
    "    collect_modifiers(h_tree, phrases, mod_type=\"NN\")\n",
    "    collect_modifiers(h_tree, phrases, mod_type=\"VB\")\n",
    "    annotation = pipeline.single_polarization(premise)\n",
    "    \n",
    "    phrasalGenerator.kb = phrases\n",
    "    phrasalGenerator.hypothesis = hypothesis.replace(',', '')\n",
    "    pp.pprint(phrasalGenerator.kb)\n",
    "    \n",
    "    polarized = pipeline.postprocess(annotation['polarized_tree'], {})\n",
    "    btreeViz = Tree.fromstring(polarized.replace('[', '(').replace(']', ')'))\n",
    "    jupyter_draw_nltk_tree(btreeViz) \n",
    "\n",
    "    polarized = pipeline.postprocess(h_tree, {})\n",
    "    btreeViz = Tree.fromstring(polarized.replace('[', '(').replace(']', ')'))\n",
    "    jupyter_draw_nltk_tree(btreeViz)\n",
    "    \n",
    "    phrasalGenerator.deptree_generate(\n",
    "        annotation['polarized_tree'], \n",
    "        annotation['annotated'], tokens)\n",
    "\n",
    "    for gen_tree in phrasalGenerator.tree_log:\n",
    "        #leaves = gen_tree[0].sorted_leaves().popkeys()\n",
    "        #sentence = ' '.join([x[0] for x in leaves])\n",
    "        print((gen_tree[1], gen_tree[2]))\n",
    "\n",
    "    print(*phrasalGenerator.sent_log, sep=\"\\n\")\n",
    "    print(phrasalGenerator.stop_critarion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.8240)"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "inference_sts(\"A yellow dog is sleeping\", \"A dog is sleeping\")"
   ]
  },
  {
   "source": [
    "## 5. Lexical Monotonicity Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import wordnet\n",
    "from wordnet import get_word_sets\n",
    "import importlib\n",
    "importlib.reload(wordnet)\n",
    "\n",
    "class LexicalGenerator:\n",
    "    def __init__(self):\n",
    "        self.deptree = None\n",
    "        self.hypothesis = \"\"\n",
    "        self.tree_log = []\n",
    "        self.anti_tree_log = []\n",
    "        self.polar_log = []\n",
    "        self.replacement_log = []\n",
    "        self.stop_critarion = False\n",
    "        self.key_tokens = [\n",
    "            'NN','NNS','NNP','NNPS','VBD',\n",
    "            'VBG','VBN','VBZ','VB',\"JJ\"]\n",
    "\n",
    "        self.propers = [\"someone\", \"something\", \"somewhere\"]\n",
    "        self.memory = {}\n",
    "\n",
    "        self.quantifiers = {}\n",
    "        self.lemmatizer = WordNetLemmatizer() \n",
    "        with open('quantifier.json', 'r') as quants:\n",
    "             quantifier_data = json.load(quants)\n",
    "             for quantifier in quantifier_data:\n",
    "                 self.quantifiers[quantifier['word']] = quantifier\n",
    "\n",
    "    def deptree_generate(self, tree):\n",
    "        self.replacement_log = []\n",
    "        self.tree_log = []\n",
    "        self.anti_tree_log = []\n",
    "        self.stop_critarion = False\n",
    "        self.deptree = tree.copy()\n",
    "        self.generate(self.deptree)\n",
    "\n",
    "    def generate(self, tree):\n",
    "        if tree is None or self.stop_critarion:\n",
    "            return\n",
    "        if tree.pos is not None and not tree.val in self.hypothesis: \n",
    "            backup = copy(tree.val)\n",
    "            if tree.pos == \"NNP\" and tree.mark == \"+\":\n",
    "                for word in self.propers:\n",
    "                    if word in self.hypothesis_tokens:\n",
    "                        tree.val = word\n",
    "                        self.save_tree()\n",
    "                        self.replacement_log.append(\n",
    "                            \"{} => {}\".format(backup, word))\n",
    "                        tree.val = backup\n",
    "\n",
    "            if tree.pos in self.key_tokens:\n",
    "                if tree.val in self.memory:\n",
    "                    hyper, hypo, syn, ant = self.memory[tree.val]\n",
    "                else:\n",
    "                    hyper, hypo, syn, ant = get_word_sets(\n",
    "                        self.lemmatizer.lemmatize(tree.val))\n",
    "                    self.memory[tree.val] = (hyper, hypo, syn, ant)\n",
    "                #print(tree.val, tree.mark, hyper)\n",
    "\n",
    "                for lex in syn.keys():\n",
    "                    lex_ls = lex.split(' ')\n",
    "                    for key in lex_ls:\n",
    "                        #print(key)\n",
    "                        #print(self.hypothesis_tokens)\n",
    "                        for tok in self.hypothesis_tokens:\n",
    "                            if tok in key or key in tok:\n",
    "                                tree.val = tok\n",
    "                                self.save_tree()\n",
    "                                self.replacement_log.append(\n",
    "                                    \"{} => {}\".format(backup, tok))\n",
    "                tree.val = backup\n",
    "\n",
    "                for lex in ant.keys():\n",
    "                    lex_ls = lex.split(' ')\n",
    "                    for key in lex_ls:\n",
    "                        #print(key)\n",
    "                        #print(self.hypothesis_tokens)\n",
    "                        for tok in self.hypothesis_tokens:\n",
    "                            if tok in key or key in tok:\n",
    "                                tree.val = tok\n",
    "                                self.save_tree(entail=False)\n",
    "                                self.replacement_log.append(\n",
    "                                    \"{} => {}\".format(backup, tok))\n",
    "                tree.val = backup\n",
    "\n",
    "                if tree.mark == \"+\":             \n",
    "                    for lex in hyper.keys():\n",
    "                        lex_ls = lex.split(' ')\n",
    "                        for key in lex_ls:\n",
    "                            #print(key)\n",
    "                            #print(self.hypothesis_tokens)\n",
    "                            for tok in self.hypothesis_tokens:\n",
    "                                if tok in key or key in tok:\n",
    "                                    tree.val = tok\n",
    "                                    self.save_tree()\n",
    "                                    self.replacement_log.append(\n",
    "                                        \"{} => {}\".format(backup, tok))\n",
    "                    tree.val = backup\n",
    "\n",
    "                if tree.mark == \"-\":\n",
    "                    for lex in hypo.keys():\n",
    "                        lex_ls = lex.split(' ')\n",
    "                        for key in lex_ls:\n",
    "                            #print(key)\n",
    "                            #print(self.hypothesis_tokens)\n",
    "                            for tok in self.hypothesis_tokens:\n",
    "                                if tok in key or key in tok:\n",
    "                                    tree.val = tok\n",
    "                                    self.save_tree()\n",
    "                                    self.replacement_log.append(\n",
    "                                        \"{} => {}\".format(backup, tok))\n",
    "                    tree.val = backup\n",
    "            \n",
    "        elif tree.val == \"det\":\n",
    "            backup = tree.left.val\n",
    "            backup_mark = tree.right.mark\n",
    "            kb = self.quantifiers.get(tree.left.val.lower(), {})\n",
    "            if len(kb) > 0:\n",
    "\n",
    "                for word in kb[\"=\"]:\n",
    "                    tree.left.val = word\n",
    "                    detType = det_type(tree.left.val)\n",
    "                    if detType is None:\n",
    "                        detType = \"det:exist\"\n",
    "                    tree.left.mark = det_mark[detType]\n",
    "                    self.save_tree()\n",
    "                    self.replacement_log.append(\n",
    "                        \"{} => {}\".format(backup, word))\n",
    "                tree.left.val = backup\n",
    "                tree.left.mark = backup_mark\n",
    "\n",
    "                if tree.left.mark == \"+\":\n",
    "                    for word in kb[\"<\"]:\n",
    "                        if word in self.hypothesis:\n",
    "                            tree.left.val = word\n",
    "                            detType = det_type(tree.left.val)\n",
    "                            if detType is None:\n",
    "                                detType = \"det:exist\"\n",
    "                            tree.left.mark = det_mark[detType]\n",
    "                            self.save_tree()\n",
    "                            self.replacement_log.append(\n",
    "                                \"{} => {}\".format(backup, word))\n",
    "                    tree.left.val = backup\n",
    "                    tree.left.mark = backup_mark\n",
    "                \n",
    "                if tree.left.mark == \"-\":\n",
    "                    for word in kb[\">\"]:\n",
    "                        if word in self.hypothesis:\n",
    "                            tree.val = word\n",
    "                            if detType is None:\n",
    "                                detType = \"det:exist\"\n",
    "                            tree.left.mark = det_mark[detType]\n",
    "                            self.save_tree()\n",
    "                            self.replacement_log.append(\n",
    "                                \"{} => {}\".format(backup, word))\n",
    "                    tree.left.val = backup\n",
    "                    tree.left.mark = backup_mark\n",
    "        \n",
    "        if tree.left != \"N\":\n",
    "            self.generate(tree.left)\n",
    "        if tree.right != \"N\":\n",
    "            self.generate(tree.right)\n",
    "\n",
    "    def save_tree(self, entail=True):\n",
    "        leaves = self.deptree.sorted_leaves().popkeys()\n",
    "        tree_copy = self.deptree.copy()\n",
    "     \n",
    "        sentence = ' '.join([x[0] for x in leaves])\n",
    "        if sentence.lower() == self.hypothesis.lower():\n",
    "            self.stop_critarion = True\n",
    "            if entail:\n",
    "                self.tree_log = []\n",
    "                self.tree_log.append((tree_copy, sentence, 1.0))\n",
    "            else:\n",
    "                self.anti_tree_log = []\n",
    "                self.anti_tree_log.append((tree_copy, sentence, 1.0))\n",
    "            return\n",
    "        \n",
    "        similarity = inference_sts([sentence], [self.hypothesis])\n",
    "        if similarity > 0.8:\n",
    "            if entail:\n",
    "                self.tree_log.append((tree_copy, sentence, similarity))\n",
    "            else:\n",
    "                self.anti_tree_log.append((tree_copy, sentence, similarity))\n",
    "        if similarity > 0.97:\n",
    "            self.stop_critarion = True\n",
    "            if entail:\n",
    "                self.tree_log = []\n",
    "                self.tree_log.append((tree_copy, sentence, similarity))\n",
    "            else:\n",
    "                self.anti_tree_log = []\n",
    "                self.anti_tree_log.append((tree_copy, sentence, similarity))\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('Two people are fighting and spectators are watching', 1.0)\n['kickboxing => fighting']\nTrue\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "sentences = [\"Two people are kickboxing and spectators are watching\"]\n",
    "hypotheses = [\"Two people are fighting and spectators are watching\"]\n",
    "\n",
    "pipeline = PolarizationPipeline(verbose=0)\n",
    "lexicalGenerator = LexicalGenerator()\n",
    "phrasalGenerator = PhrasalGenerator()\n",
    "\n",
    "for premise, hypothesis in zip(sentences, hypotheses):\n",
    "    premise = phrasalGenerator.preprocess(premise)\n",
    "    hypothesis = phrasalGenerator.preprocess(hypothesis)\n",
    "\n",
    "    h_parsed, replaced = dependency_parse(hypothesis, parser=\"stanza\")\n",
    "    #h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "    #pipeline.modify_replacement(h_tree, replaced)\n",
    "    key_tokens = set()\n",
    "    for word in h_parsed[2]:\n",
    "        pos = h_parsed[2][word][1]\n",
    "        if 'NN' in pos or 'JJ' in pos or 'VB' in pos:\n",
    "            key_tokens.add(h_parsed[2][word][0])\n",
    "\n",
    "    #print(\"\\n====================================\")\n",
    "    #print(\"\\nInit Premise: \" + premise)\n",
    "    #print(\"\\nHypothesis: \" + hypothesis)\n",
    "\n",
    "    #tokenized = tokenizer(hypothesis).sentences[0].words\n",
    "    #tokens = {} \n",
    "    #lemmatizer = WordNetLemmatizer() \n",
    "    #for tok in tokenized:\n",
    "    #    tokens[lemmatizer.lemmatize(tok.text)] = tok.text\n",
    "    lexicalGenerator.hypothesis_tokens = key_tokens\n",
    "\n",
    "    annotation = pipeline.single_polarization(premise)\n",
    "    polarized = pipeline.postprocess(annotation['polarized_tree'], {})\n",
    "    btreeViz = Tree.fromstring(polarized.replace('[', '(').replace(']', ')'))\n",
    "    jupyter_draw_nltk_tree(btreeViz) \n",
    "\n",
    "    lexicalGenerator.hypothesis = hypothesis.replace(',', '')\n",
    "    lexicalGenerator.deptree_generate(annotation['polarized_tree'])\n",
    "    \n",
    "    for gen_tree in lexicalGenerator.tree_log:\n",
    "        print((gen_tree[1], gen_tree[2]))\n",
    "    for anti_tree in lexicalGenerator.anti_tree_log:\n",
    "        print((anti_tree[1], anti_tree[2]))\n",
    "\n",
    "    print(lexicalGenerator.replacement_log)\n",
    "    print(lexicalGenerator.stop_critarion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "458\n1738\n"
     ]
    }
   ],
   "source": [
    "MED_upward = []\n",
    "MED_upward_hypo = []\n",
    "MED_downward = []\n",
    "MED_downward_hypo = []\n",
    "\n",
    "with open(\"../data/MED/upward.txt\") as upward_med:\n",
    "    lines = upward_med.readlines()\n",
    "    for i in range(len(lines) // 4):\n",
    "        MED_upward.append(lines[i*4+1])\n",
    "        MED_upward_hypo.append(lines[i*4+2])\n",
    "\n",
    "with open(\"../data/MED/downward.txt\") as donward_med:\n",
    "    lines = donward_med.readlines()\n",
    "    for i in range(len(lines) // 4):\n",
    "        MED_downward.append(lines[i*4+1])\n",
    "        MED_downward_hypo.append(lines[i*4+2])\n",
    "\n",
    "print(len(MED_upward))\n",
    "print(len(MED_downward))"
   ]
  },
  {
   "source": [
    "## 6. Syntactic Variational Inference"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunker import Chunker\n",
    "\n",
    "class SyntacticVariator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chunker = Chunker()\n",
    "        self.paraphraseTokenizer = paraphraseTokenizer\n",
    "        self.paraphraseModel = paraphraseModel\n",
    "\n",
    "    def chunking(self, tree):\n",
    "        return self.chunker.get_chunks_byDepTree(tree)\n",
    "\n",
    "    def build_pairs(self, chunks1, chunks2):\n",
    "        chunk_pairs = []\n",
    "        for chunk1 in chunks1:\n",
    "            for chunk2 in chunks2:\n",
    "                if len(set(chunk1.split(' ')).intersection(chunk2.split(' '))) > 0:\n",
    "                     chunk_pairs.append((chunk1, chunk2))\n",
    "\n",
    "        return chunk_pairs\n",
    "\n",
    "    def inference_mrpc(self, seq1, seq2):\n",
    "        paraphrase = paraphraseTokenizer.encode_plus(\n",
    "            seq1, seq2, return_tensors=\"pt\")\n",
    "        paraphrase.to('cuda')\n",
    "        logits = paraphraseModel(**paraphrase)[0]\n",
    "        paraphrase_results = torch.softmax(logits, dim=1).tolist()[0]\n",
    "        return paraphrase_results[1]\n",
    "\n",
    "    def phrase_alignment(self, chunk_pairs):\n",
    "        alignments = []\n",
    "        for pair in chunk_pairs:\n",
    "            score = self.inference_mrpc(pair[0], pair[1])\n",
    "            #print(pair, score)\n",
    "            if score > 0.80:\n",
    "                alignments.append(pair)\n",
    "\n",
    "        return alignments\n",
    "\n",
    "    def variate(self, P, H, p_tree, h_tree, sent=False):\n",
    "        p_chunks = self.chunking(p_tree)\n",
    "        h_chunks = self.chunking(h_tree)\n",
    "\n",
    "        ie_pred_p = ie_extractor.predict(P)['verbs']\n",
    "        ie_pred_h = ie_extractor.predict(H)['verbs']\n",
    "\n",
    "        for verb in ie_pred_p:\n",
    "            if \"ARG\" in verb['description']:\n",
    "                p_chunks.append(fix_info(verb['description'])[0].strip())\n",
    "                p_chunks.append(verb['verb'] + ' '+ fix_info(verb['description'])[2].strip())\n",
    "\n",
    "        for verb in ie_pred_h:\n",
    "            if \"ARG\" in verb['description']:\n",
    "                h_chunks.append(fix_info(verb['description'])[0].strip())\n",
    "                h_chunks.append(verb['verb'] + ' '+ fix_info(verb['description'])[2].strip())\n",
    "\n",
    "        if sent:\n",
    "            p_chunks.append(P)\n",
    "            h_chunks.append(H)\n",
    "\n",
    "        chunk_pairs = self.build_pairs(p_chunks, h_chunks)\n",
    "        alignments = self.phrase_alignment(chunk_pairs)\n",
    "\n",
    "        #print(*p_chunks, sep=\"\\n\")\n",
    "        #print(*h_chunks, sep=\"\\n\")\n",
    "\n",
    "        variates = set()\n",
    "        for align in alignments:\n",
    "            var_sentence = P.replace(align[0], align[1])\n",
    "            variates.add(var_sentence)\n",
    "\n",
    "        return variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpdx7xqd4h\\config.json as plain json\n",
      "Spacy models 'en_core_web_sm' not found.  Downloading and installing.\n",
      " Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.structured_prediction\n",
    "\n",
    "ie_extractor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_info(desc):\n",
    "    out = desc.replace(\"[ARG0: \", \"\")\n",
    "    out = out.replace(\"[ARG1: \", \"\")\n",
    "    out = out.replace(\"[V: \", \"\")\n",
    "    out = out.replace(\"]\", \",\")\n",
    "    out = out.split(\",\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'verb': 'are', 'description': 'Two men [V: are] standing near the water and are holding fishing poles', 'tags': ['O', 'O', 'B-V', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'verb': 'standing', 'description': '[ARG1: Two men] are [V: standing] [ARG2: near the water] and are holding fishing poles', 'tags': ['B-ARG1', 'I-ARG1', 'O', 'B-V', 'B-ARG2', 'I-ARG2', 'I-ARG2', 'O', 'O', 'O', 'O', 'O']}, {'verb': 'are', 'description': 'Two men are standing near the water and [V: are] holding fishing poles', 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'O', 'O', 'O']}, {'verb': 'holding', 'description': '[ARG0: Two men] are standing near the water and are [V: holding] [ARG1: fishing poles]', 'tags': ['B-ARG0', 'I-ARG0', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'B-ARG1', 'I-ARG1']}]\n['Two men', ' are standing', ' [ARG2: near the water', ' and are holding fishing poles']\n['Two men', ' are standing near the water and are holding', ' fishing poles', '']\n"
     ]
    }
   ],
   "source": [
    "ie_pred = ie_extractor.predict(\"Two men are standing near the water and are holding fishing poles\")['verbs']\n",
    "print(ie_pred)\n",
    "for verb in ie_pred:\n",
    "    if \"ARG\" in verb['description']:\n",
    "        print(fix_info(verb['description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Two men near the water and are holding fishing poles tensor(0.7851)\nTwo men are standing near the water and are holding tools used for fishing tensor(1.0000)\nTwo men are standing near the water and are holding fishing poles tensor(0.8147)\nTwo men are standing are standing near the water and are holding fishing poles tensor(0.7937)\n"
     ]
    }
   ],
   "source": [
    "premise = \"Two men are standing near the water and are holding fishing poles\"\n",
    "hypothesis = \"Two men are standing near the water and are holding tools used for fishing\"\n",
    "\n",
    "pipeline = PolarizationPipeline()\n",
    "syntacticVariator = SyntacticVariator()\n",
    "\n",
    "h_parsed, replaced = dependency_parse(hypothesis, parser=\"stanza\")\n",
    "h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "pipeline.modify_replacement(h_tree, replaced)\n",
    "annotation = pipeline.single_polarization(premise)\n",
    "\n",
    "variates = syntacticVariator.variate(premise, hypothesis, annotation['polarized_tree'],  h_tree, sent=False)\n",
    "for v in variates:\n",
    "    similarity = inference_sts([v], [hypothesis])\n",
    "    print(v, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7883373498916626"
      ]
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "syntacticVariator.inference_mrpc(\"A group of scouts are hiking\", \"People are walking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.7228)"
      ]
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "source": [
    "inference_sts(\"A group of people are hiking\", \"People are walking\")"
   ]
  },
  {
   "source": [
    "## 7. A* Inference Search Engine"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pqdict import pqdict\n",
    "\n",
    "class AStarPlanner:\n",
    "    def __init__(self):    \n",
    "        self.closed = []                  \n",
    "        self.entailments = set()\n",
    "        self.contradictions = set()\n",
    "        self.hypothesis = \"\"\n",
    "        self.h_tree = None\n",
    "\n",
    "        self.pipeline = PolarizationPipeline()\n",
    "        self.phrasalGenerator = PhrasalGenerator()\n",
    "        self.lexicalGenerator = LexicalGenerator()\n",
    "        self.syntacticVariator = SyntacticVariator() \n",
    "\n",
    "    def hypothesis_kb(self):\n",
    "        self.hypothesis = self.phrasalGenerator.preprocess(self.hypothesis)\n",
    "        h_parsed, replaced = dependency_parse(self.hypothesis, parser=\"stanza\")\n",
    "        h_tree, _ = self.pipeline.run_binarization(h_parsed, self.hypothesis, {})\n",
    "        self.pipeline.modify_replacement(h_tree, replaced)\n",
    "        phrases = {} \n",
    "        collect_modifiers(h_tree, phrases, mod_type=\"NN\")\n",
    "        collect_modifiers(h_tree, phrases, mod_type=\"VB\")\n",
    "        self.phrasalGenerator.kb = phrases\n",
    "        self.h_tree = h_tree\n",
    "\n",
    "    def generate_premises(self, start):\n",
    "        self.entailments.clear()\n",
    "        self.contradictions.clear()\n",
    "\n",
    "        # Polarization from Udeo2Mono\n",
    "        start = self.phrasalGenerator.preprocess(start)\n",
    "        annotation = self.pipeline.single_polarization(start)\n",
    "\n",
    "        # Monotonicity-based Phrasl Inference\n",
    "        self.phrasalGenerator.hypothesis = self.hypothesis.replace(',', '')\n",
    "\n",
    "        tokenized = tokenizer(start).sentences[0].words\n",
    "        tokens = [tok.text for tok in tokenized]\n",
    "\n",
    "        self.phrasalGenerator.deptree_generate(\n",
    "        annotation['polarized_tree'], \n",
    "        annotation['annotated'], tokens)\n",
    "        #print(\"============================\")\n",
    "        \n",
    "        if self.phrasalGenerator.stop_critarion:\n",
    "            return True\n",
    "\n",
    "        for tree in self.phrasalGenerator.tree_log:\n",
    "            self.entailments.add((tree[1], tree[2]))\n",
    "        self.entailments |= set(self.phrasalGenerator.sent_log)\n",
    "\n",
    "        #print(*self.entailments, sep=\"\\n\")\n",
    "\n",
    "        # Syntactic Vriation\n",
    "        # Sequence Chunking and Chunk Alignment from roBERTa\n",
    "        sent_level = False\n",
    "        if self.current_optimal > 0.93 :\n",
    "            sent_level = True\n",
    "        variates = self.syntacticVariator.variate(\n",
    "            start, \n",
    "            self.hypothesis, \n",
    "            annotation['polarized_tree'], \n",
    "            self.h_tree, sent_level)\n",
    "        for v in variates:\n",
    "            similarity = inference_sts([v], [self.hypothesis])\n",
    "            if similarity > 0.98:\n",
    "                return True\n",
    "            elif similarity > 0.9:\n",
    "                self.entailments.add((v, similarity))\n",
    "\n",
    "        #TODO: Monotonicity-based Lexical Inference\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        tokens_dict = {}\n",
    "        for tok in tokenized:\n",
    "            tokens_dict[lemmatizer.lemmatize(tok.text)] = tok.text\n",
    "        self.lexicalGenerator.hypothesis_tokens = tokens_dict\n",
    "        self.lexicalGenerator.hypothesis = self.hypothesis.replace(',', '')\n",
    "        \n",
    "        self.lexicalGenerator.deptree_generate(annotation['polarized_tree'])\n",
    "        if self.lexicalGenerator.stop_critarion:\n",
    "            return True\n",
    "        for tree in self.lexicalGenerator.tree_log:\n",
    "            #print((tree[1], tree[2]))\n",
    "            self.entailments.add((tree[1], tree[2]))\n",
    "\n",
    "        return False\n",
    "\n",
    "    def generate(self, start, opened):\n",
    "        terminate = self.generate_premises(start)\n",
    "        if terminate:\n",
    "            return True\n",
    "\n",
    "        for premise in self.entailments:\n",
    "            if premise in self.closed:\n",
    "                continue\n",
    "            cost = premise[1]\n",
    "            if premise[0] not in opened:\n",
    "                opened[premise[0]] = cost\n",
    "            if cost < opened[premise[0]]:\n",
    "                opened[premise[0]] = cost\n",
    "        return False\n",
    "\n",
    "    def search(self, premises, hypothesis):\n",
    "        self.closed = pqdict({})\n",
    "        self.hypothesis = hypothesis\n",
    "\n",
    "        self.hypothesis_kb()\n",
    "        self.phrasalGenerator.hypothesis = self.hypothesis\n",
    "        self.lexicalGenerator.hypothesis = self.hypothesis\n",
    "\n",
    "        open_lists = pqdict({}, reverse=True)\n",
    "        open_lists[premises] = inference_sts([premises], [hypothesis])\n",
    "\n",
    "        hop = 0\n",
    "        top_k = 2\n",
    "\n",
    "        while open_lists:\n",
    "            for _ in range(top_k):\n",
    "                if len(open_lists) > 0:\n",
    "                    optimal = open_lists.popitem()\n",
    "                    self.current_optimal = optimal[1]\n",
    "                    #print(\"Optimal: \", optimal)\n",
    "                    goal_found = self.generate(optimal[0], open_lists)\n",
    "                    self.closed[optimal] = len(self.closed) + 1\n",
    "                    if goal_found:\n",
    "                        self.closed[(self.hypothesis, 1.0)] = len(self.closed) + 1\n",
    "                        return True\n",
    "                else: break   \n",
    "            hop += 1\n",
    "            if hop > 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = AStarPlanner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/1414 [00:00<?, ?it/s]1414\n",
      "100%|| 1414/1414 [31:29<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "entail_p = []\n",
    "entail_hypo = []\n",
    "with open(\"../data/SICk/entail.txt\") as upward_med:\n",
    "    lines = upward_med.readlines()\n",
    "    for i in range(len(lines) // 3):\n",
    "        entail_p.append(lines[i*3])\n",
    "        entail_hypo.append(lines[i*3+1])\n",
    "\n",
    "print(len(entail_p))\n",
    "\n",
    "with open(\"./generation_log_SICK.txt\", 'w') as generate_log:\n",
    "    for i in tqdm(range(0, len(entail_p))):\n",
    "        premise = entail_p[i].replace('\\n', '')\n",
    "        hypothesis = entail_hypo[i].replace('\\n', '')\n",
    "        try:\n",
    "            entail = planner.search(premise, hypothesis)\n",
    "            if not entail:\n",
    "                generate_log.write(\"\\nID: \" + str(i))\n",
    "                generate_log.write(\"\\nPremise: \" + premise)\n",
    "                generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "                generate_log.write('\\n')\n",
    "        except:\n",
    "            generate_log.write(\"\\nID: \" + str(i))\n",
    "            generate_log.write(\"\\nPremise: \" + premise)\n",
    "            generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "            generate_log.write('\\n')\n",
    "\n",
    "        #print(*planner.closed, sep=\" =>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('Tambourines are being played by a group of children', tensor(0.9701)) =>\n('A group of children is playing tambourines', 1.0)\nTrue\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"Tambourines are being played by a group of children\", \n",
    "             \"A group of children is playing tambourines\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimal:  ('A family is watching a little boy who is hitting a baseball', tensor(0.7123))\n",
      "============================\n",
      "Optimal:  ('a little boy who is hitting a baseball', tensor(0.9348))\n",
      "============================\n",
      "('A family is watching a little boy who is hitting a baseball', tensor(0.7123)) =>\n",
      "('a little boy who is hitting a baseball', tensor(0.9348)) =>\n",
      "('A child is hitting a baseball', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"A family is watching a little boy who is hitting a baseball\", \n",
    "             \"A child is hitting a baseball\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimal:  ('You know that some life changing actions must be taken when grandma reacts with the sad emoji', tensor(0.9705))\n",
      "============================\n",
      "('You know that some life changing actions must be taken when grandma reacts with the sad emoji', tensor(0.9705)) =>\n",
      "('You know that some actions must be taken when grandma reacts with the sad emoji', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"You know that some life changing actions must be taken when grandma reacts with the sad emoji\", \"You know that some actions must be taken when grandma reacts with the sad emoji\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimal:  ('A black and a white dog are joyfully running on the grass', tensor(0.6965))\n",
      "============================\n",
      "Optimal:  ('A black and a white dog are running on the grass', tensor(0.9178))\n",
      "============================\n",
      "Optimal:  ('A black and a white dog are running on the grass', tensor(0.9372))\n",
      "============================\n",
      "('A black and a white dog are joyfully running on the grass', tensor(0.6965)) =>\n",
      "('A black and a white dog are running on the grass', tensor(0.9178)) =>\n",
      "('A black and a white dog are running on the grass', tensor(0.9372)) =>\n",
      "('A dog, which has a black coat, and a white dog are running on the grass', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"A black and a white dog are joyfully running on the grass\", \n",
    "             \"A dog, which has a black coat, and a white dog are running on the grass\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('a blue motorcycle', 'a motorbike') 0.3998011350631714\n",
      "('a blue motorcycle', 'along a roadway') 0.022760460153222084\n",
      "('a blue motorcycle', 'is riding a motorbike') 0.4973074793815613\n",
      "('is riding a blue motorcycle', 'a motorbike') 0.6232423782348633\n",
      "('is riding a blue motorcycle', 'along a roadway') 0.0883413553237915\n",
      "('is riding a blue motorcycle', 'is riding a motorbike') 0.9724442958831787\n",
      "('is riding a blue motorcycle', 'is riding') 0.7689359188079834\n",
      "('with a red helmet', 'a motorbike') 0.019708052277565002\n",
      "('with a red helmet', 'along a roadway') 0.017485572025179863\n",
      "('with a red helmet', 'is riding a motorbike') 0.02164103090763092\n",
      "('A man with a red helmet', 'a motorbike') 0.0247005857527256\n",
      "('A man with a red helmet', 'along a roadway') 0.04748450592160225\n",
      "('A man with a red helmet', 'is riding a motorbike') 0.2289326936006546\n",
      "('A man with a red helmet', 'A motorcyclist') 0.07697919756174088\n",
      "('is riding', 'is riding a motorbike') 0.9855220317840576\n",
      "('is riding', 'is riding') 0.9183151721954346\n",
      "('A man', 'A motorcyclist') 0.04921848699450493\n",
      "('a blue motorcycle', 'a motorbike') 0.3998011350631714\n",
      "('a blue motorcycle', 'along a roadway') 0.022760460153222084\n",
      "('a blue motorcycle', 'is riding a motorbike') 0.4973074793815613\n",
      "('is riding a blue motorcycle', 'a motorbike') 0.6232423782348633\n",
      "('is riding a blue motorcycle', 'along a roadway') 0.0883413553237915\n",
      "('is riding a blue motorcycle', 'is riding a motorbike') 0.9724442958831787\n",
      "('is riding a blue motorcycle', 'is riding') 0.7689359188079834\n",
      "('is riding', 'is riding a motorbike') 0.9855220317840576\n",
      "('is riding', 'is riding') 0.9183151721954346\n",
      "('A man', 'A motorcyclist') 0.04921848699450493\n",
      "('a motorbike', 'a motorbike') 0.9887796640396118\n",
      "('a motorbike', 'along a roadway') 0.032046977430582047\n",
      "('a motorbike', 'is riding a motorbike') 0.9866973757743835\n",
      "('is riding a motorbike', 'a motorbike') 0.9453105330467224\n",
      "('is riding a motorbike', 'along a roadway') 0.050036754459142685\n",
      "('is riding a motorbike', 'is riding a motorbike') 0.9970396161079407\n",
      "('is riding a motorbike', 'is riding') 0.9621288180351257\n",
      "('is riding', 'is riding a motorbike') 0.9855220317840576\n",
      "('is riding', 'is riding') 0.9183151721954346\n",
      "('A man', 'A motorcyclist') 0.04921848699450493\n",
      "('a motorbike', 'a motorbike') 0.9887796640396118\n",
      "('a motorbike', 'along a roadway') 0.032046977430582047\n",
      "('a motorbike', 'is riding a motorbike') 0.9866973757743835\n",
      "('is riding a motorbike', 'a motorbike') 0.9453105330467224\n",
      "('is riding a motorbike', 'along a roadway') 0.050036754459142685\n",
      "('is riding a motorbike', 'is riding a motorbike') 0.9970396161079407\n",
      "('is riding a motorbike', 'is riding') 0.9621288180351257\n",
      "('is riding', 'is riding a motorbike') 0.9855220317840576\n",
      "('is riding', 'is riding') 0.9183151721954346\n",
      "('A man with a red helmet is riding a blue motorcycle down the road', tensor(0.6542)) =>\n",
      "('A man is riding a blue motorcycle down the road', tensor(0.7366)) =>\n",
      "('A man is riding a motorbike down the road', tensor(0.9153)) =>\n",
      "('A man is riding a motorbike a motorbike down the road', tensor(0.9292)) =>\n",
      "('A man is riding is riding a motorbike is riding a motorbike down the road', tensor(0.9325)) =>\n",
      "('A motorcyclist is riding a motorbike along a roadway', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"A man with a red helmet is riding a blue motorcycle down the road\", \n",
    "             \"A motorcyclist is riding a motorbike along a roadway\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Two men are holding fishing poles tensor(0.6642)\n",
      "Two men are holding fishing poles tensor(0.6642)\n",
      "Two men are holding fishing poles tensor(0.6642)\n",
      "Two men are standing near the water and are holding fishing tensor(0.8630)\n",
      "Two men are standing near the water and are holding fishing tensor(0.8630)\n",
      "Two men are standing near the water and are holding poles tensor(0.6653)\n",
      "Two men are standing near the water and are holding poles tensor(0.6653)\n",
      "Two men are standing and are holding fishing poles tensor(0.7028)\n",
      "Two men are standing and are holding fishing poles tensor(0.7028)\n",
      "Two men are standing near water and are holding fishing poles tensor(0.8203)\n",
      "Two men are standing near water and are holding fishing poles tensor(0.8203)\n",
      "Two men are standing near the water tensor(0.7110)\n",
      "Two men are standing near the water tensor(0.7110)\n",
      "Two men are standing near the water and are holding fishing tensor(0.8630)\n",
      "Two men are standing near the water and are holding fishing tensor(0.8630)\n",
      "Two men are standing near the water and are holding poles tensor(0.6653)\n",
      "Two men are standing near the water and are holding poles tensor(0.6653)\n",
      "Two men are standing and are holding fishing poles tensor(0.7028)\n",
      "Two men are standing and are holding fishing poles tensor(0.7028)\n",
      "Two men are standing near water and are holding fishing poles tensor(0.8203)\n",
      "Two men are standing near water and are holding fishing poles tensor(0.8203)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(entail_p[55], entail_hypo[55])\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimal:  ('A family is watching a little boy who is hitting a baseball', tensor(0.7123))\n",
      "============================\n",
      "Optimal:  ('a little boy who is hitting a baseball', tensor(0.9348))\n",
      "============================\n",
      "('A family is watching a little boy who is hitting a baseball', tensor(0.7123)) =>\n",
      "('a little boy who is hitting a baseball', tensor(0.9348)) =>\n",
      "('A child is hitting a baseball', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"A family is watching a little boy who is hitting a baseball\", \n",
    "             \"A child is hitting a baseball\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimal:  ('A deer is jumping over a fence', tensor(0.9798))\n",
      "============================\n",
      "('A deer is jumping over a fence', tensor(0.9798)) =>\n",
      "('A deer is jumping a fence', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"A deer is jumping over a fence\", \n",
    "             \"A deer is jumping a fence\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimal:  ('A boy is hitting a baseball', tensor(1.0000))\n",
      "============================\n",
      "('A boy is hitting a baseball', tensor(1.0000)) =>\n",
      "('A boy is hitting a baseball', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"A boy is hitting a baseball\", \n",
    "             \"A chil is hitting a baseball\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimal:  ('A brown dog is attacking another animal in front of the tall man in pants', tensor(0.8783))\n",
      "============================\n",
      "Optimal:  ('A dog is attacking another animal in front of the tall man in pants', tensor(0.9582))\n",
      "============================\n",
      "('A brown dog is attacking another animal in front of the tall man in pants', tensor(0.8783)) =>\n",
      "('A dog is attacking another animal in front of the tall man in pants', tensor(0.9582)) =>\n",
      "('A dog is attacking another animal in front of the man in pants', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"A brown dog is attacking another animal in front of the tall man in pants\", \n",
    "             \"A dog is attacking another animal in front of the man in pants\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimal:  ('A family is watching a little boy who is hitting a baseball', tensor(0.9915))\n",
      "============================\n",
      "('A family is watching a little boy who is hitting a baseball', tensor(0.9915)) =>\n",
      "('A family is watching a boy who is hitting a baseball', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"A family is watching a little boy who is hitting a baseball\", \n",
    "             \"A family is watching a boy who is hitting a baseball\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimal:  (\"You can't park in front of my house on weekends.\", tensor(0.9529))\n",
      "============================\n",
      "(\"You can't park in front of my house on weekends.\", tensor(0.9529)) =>\n",
      "('You can not park in front of my large house on weekends', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"You can't park in front of my house on weekends.\", \n",
    "             \"You can't park in front of my large house on weekends.\")\n",
    "\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "annotations = []\n",
    "with open(\"./generation_log_upward.txt\", 'w') as generate_log:\n",
    "    phrasalGenerator = PhrasalGenerator()\n",
    "    pipeline = PolarizationPipeline(verbose=0)\n",
    "    for i in tqdm(range(0, 500)):\n",
    "        premise = MED_none[i].replace('\\n', '')\n",
    "        hypothesis = MED_none_hypo[i].replace('\\n', '')\n",
    "        premise = phrasalGenerator.preprocess(premise)\n",
    "        hypothesis = phrasalGenerator.preprocess(hypothesis)\n",
    "\n",
    "        tokenized = tokenizer(premise).sentences[0].words\n",
    "        tokens = [tok.text for tok in tokenized]\n",
    "\n",
    "        try:\n",
    "            h_parsed, replaced = dependency_parse(hypothesis, parser=\"stanza\")\n",
    "            h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "        except:\n",
    "            generate_log.write(\"\\nPremise: \" + premise)\n",
    "            generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "            continue\n",
    "        pipeline.modify_replacement(h_tree, replaced)\n",
    "        phrases = {} \n",
    "        collect_modifiers(h_tree, phrases, mod_type=\"NN\")\n",
    "        collect_modifiers(h_tree, phrases, mod_type=\"VB\")\n",
    "\n",
    "        try:\n",
    "            annotation = pipeline.single_polarization(premise)\n",
    "        except:\n",
    "            #generate_log.write(\"\\nPremise: \" + premise)\n",
    "            #generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "            continue\n",
    "    \n",
    "        phrasalGenerator.kb = phrases\n",
    "        #print(phrasalGenerator.kb)\n",
    "        phrasalGenerator.hypothesis = hypothesis.replace(',', '')\n",
    "        \n",
    "        phrasalGenerator.deptree_generate(\n",
    "            annotation['polarized_tree'], \n",
    "            annotation['annotated'], \n",
    "            tokens)\n",
    "\n",
    "        # for gen_tree in phrasalGenerator.tree_log:\n",
    "        #    leaves = gen_tree[0].sorted_leaves().popkeys()\n",
    "        #    sentence = ' '.join([x[0] for x in leaves])\n",
    "        #    print((sentence, gen_tree[1]))\n",
    "            \n",
    "        if phrasalGenerator.stop_critarion:\n",
    "            generate_log.write(\"\\nID: \" + str(i))\n",
    "            generate_log.write(\"\\nPremise: \" + premise)\n",
    "            generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "            #print(\"\\nPremise: \" + premise)\n",
    "            #print(\"\\nHypothesis: \" + hypothesis)\n",
    "            #print(*phrasalGenerator.sent_log, sep=\"\\n\")\n",
    "            #generate_log.writelines(phrasalGenerator.sent_log)\n",
    "            generate_log.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}