{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UdepLog Neural-Logical Inference System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tree import Tree\n",
    "from nltk.draw import TreeWidget\n",
    "from nltk.draw.util import CanvasFrame\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def jupyter_draw_nltk_tree(tree):\n",
    "    cf = CanvasFrame()\n",
    "    tc = TreeWidget(cf.canvas(), tree)\n",
    "    tc['node_font'] = 'arial 14 bold'\n",
    "    tc['leaf_font'] = 'ar|ial 14'\n",
    "    tc['node_color'] = '#005990'\n",
    "    tc['leaf_color'] = '#3F8F57'\n",
    "    tc['line_color'] = '#175252'\n",
    "    cf.add_widget(tc, 20, 20)\n",
    "    os.system('rm -rf ../data/tree.png')\n",
    "    os.system('rm -rf ../data/tree.ps')\n",
    "    cf.print_to_file('../data/tree.ps')\n",
    "    cf.destroy()\n",
    "    os.system('convert ../data/tree.ps ../data/tree.png')\n",
    "    display(Image(filename='../data/tree.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BERT Model for Pharaphrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Load Alignment Model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "roberta_MRPC = \"textattack/roberta-base-MRPC\"\n",
    "bert_MRPC = \"bert-base-cased-finetuned-mrpc\"\n",
    "albert_MRPC = \"textattack/albert-base-v2-MRPC\"\n",
    "\n",
    "paraphraseTokenizer = AutoTokenizer.from_pretrained(albert_MRPC)  \n",
    "paraphraseModel = AutoModelForSequenceClassification.from_pretrained(albert_MRPC)\n",
    "paraphraseModel.to('cuda')\n",
    "print(\"Load Alignment Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_STS = \"bert-base-uncased-STS-B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. UD Parser and RoBERTa Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-04-06 13:58:01 INFO: Loading these models for language: en (English):\n",
      "========================================\n",
      "| Processor | Package                  |\n",
      "----------------------------------------\n",
      "| tokenize  | ../model/e...ize/gum.pt  |\n",
      "| pos       | ../model/en/pos/ewt.pt   |\n",
      "| lemma     | ../model/en/lemma/gum.pt |\n",
      "| depparse  | ../model/e...rse/gum.pt  |\n",
      "========================================\n",
      "\n",
      "2021-04-06 13:58:01 INFO: Use device: gpu\n",
      "2021-04-06 13:58:01 INFO: Loading: tokenize\n",
      "2021-04-06 13:58:01 INFO: Loading: pos\n",
      "2021-04-06 13:58:02 INFO: Loading: lemma\n",
      "2021-04-06 13:58:02 INFO: Loading: depparse\n",
      "2021-04-06 13:58:03 INFO: Done loading processors!\n",
      "2021-04-06 13:58:03 INFO: Loading these models for language: en (English):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | ../model/e...ize/gum.pt |\n",
      "=======================================\n",
      "\n",
      "2021-04-06 13:58:03 INFO: Use device: cpu\n",
      "2021-04-06 13:58:03 INFO: Loading: tokenize\n",
      "2021-04-06 13:58:03 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from wordnet import *\n",
    "from copy import deepcopy\n",
    "from Udep2Mono.util import det_mark, det_type\n",
    "from Udep2Mono.util import btree2list\n",
    "from Udep2Mono.dependency_parse import tokenizer\n",
    "from Udep2Mono.dependency_parse import dependency_parse\n",
    "from Udep2Mono.binarization import BinaryDependencyTree\n",
    "from Udep2Mono.polarization import PolarizationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "sentenceTransformer = SentenceTransformer(\"stsb-bert-large\")\n",
    "sentenceTransformer.to('cuda')\n",
    "from nltk.corpus import stopwords\n",
    "ign_words = dict()\n",
    "for word in stopwords.words('english'):\n",
    "    ign_words[word] = 1\n",
    "\n",
    "def inference_sts(seq1s, seq2s, dist=False):\n",
    "    embeddings1 = sentenceTransformer.encode(seq1s, convert_to_tensor=True)\n",
    "    embeddings2 = sentenceTransformer.encode(seq2s, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "    distance = torch.dist(embeddings1, embeddings2, p=2)\n",
    "    if dist:\n",
    "        return distance\n",
    "    return cosine_scores[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phrasal Monotonicity Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import pluralize, singularize\n",
    "from copy import copy\n",
    "import re\n",
    "import torch\n",
    "\n",
    "class PhrasalGenerator:\n",
    "    def __init__(self):\n",
    "        self.deptree = None\n",
    "        self.annotated = None\n",
    "        self.original = None\n",
    "        self.kb = {}\n",
    "        self.hypothesis = \"\"\n",
    "        self.tree_log = []\n",
    "        self.sent_log = []\n",
    "        self.stop_critarion = False\n",
    "        self.mod_at_left = [\n",
    "            \"advmod\", \"amod\", \"advmod:count\", \n",
    "            \"acl:relcl\", \"obl\", 'obl:npmod', \"det\",\n",
    "            \"obl:tmod\", \"nmod\", \"nmod:npmod\", \n",
    "            \"nmod:poss\", \"nmod:tmod\", \"obl:npmod\",\n",
    "            \"acl\", \"advcl\", \"xcomp\", \"ccomp\", \n",
    "            'compound:ptr']\n",
    "        self.mod_at_right = [\"appos\"] #\"obj\"\n",
    "        self.mod_symmetric = [\"conj\", \"compound\"]\n",
    "        self.mod_special = [\"nsubj\"]\n",
    "        self.implicative = {\n",
    "            \"watching\": 1\n",
    "        }\n",
    "        \n",
    "        '''  \n",
    "            \"cop\": self.generate_inherite, \n",
    "            \"expl\": self.generate_expl,\n",
    "            \"nummod\": self.generate_nummod,\n",
    "        '''\n",
    "\n",
    "    def deptree_generate(self, tree, annotated, original):\n",
    "        self.stop_critarion = False\n",
    "        self.tree_log = []\n",
    "        self.sent_log = []\n",
    "        self.deptree = tree.copy()\n",
    "        self.original = original  \n",
    "        self.annotated = deepcopy(annotated)\n",
    "        self.sentence = original\n",
    "        self.generate(self.deptree)\n",
    "\n",
    "    def generate(self, tree):\n",
    "        if self.stop_critarion:\n",
    "            return\n",
    "        if not tree.is_tree:\n",
    "            self.generate_default(tree)\n",
    "        else:\n",
    "            generation = self.get_generation_type(tree)\n",
    "            #print(generation, tree.val)\n",
    "            generation(tree)\n",
    "    def removeGroup(self, tree):\n",
    "        if(tree.val == \"nmod\"):\n",
    "            if(tree.right.is_tree and tree.right.left.val.lower() == \"a\"):\n",
    "                if(tree.left.is_tree and tree.left.left.val.lower() == \"of\"):\n",
    "                    noun = tree.left.right\n",
    "                    group = tree.right.right\n",
    "                    self.delete_modifier(tree, noun)\n",
    "                    self.delete_modifier(tree, group)\n",
    "                    return True\n",
    "        return False\n",
    "    def get_generation_type(self, tree):\n",
    "        if tree.val in self.mod_special:\n",
    "            return self.generate_special\n",
    "\n",
    "        disjunction = False\n",
    "        if tree.val == \"conj\":\n",
    "            disjunction |= self.search_dependency('or', tree.left)\n",
    "            disjunction |= self.search_dependency('and', tree.left)\n",
    "        \n",
    "        left_mod = tree.left.mark == \"+\"\n",
    "        left_mod = left_mod or tree.left.mark == \"=\" or disjunction\n",
    "        left_mod = left_mod and tree.val in self.mod_at_left\n",
    "\n",
    "        right_mod = tree.right.mark == \"+\" or tree.right.mark == \"=\" or disjunction \n",
    "        right_mod = right_mod and tree.val in self.mod_at_right\n",
    "\n",
    "        sym_mod = tree.val in self.mod_symmetric and tree.left.mark == \"+\" and tree.right.mark == \"+\"\n",
    "        \n",
    "        if left_mod:\n",
    "            return self.left_modifier_generate\n",
    "        elif right_mod:\n",
    "            return self.right_modifier_generate\n",
    "        elif sym_mod:\n",
    "            return self.symmetric_generate\n",
    "        else:\n",
    "            return self.generate_default\n",
    "\n",
    "    def generate_special(self, tree):\n",
    "        if tree.val == \"nsubj\":\n",
    "            if tree.left.val == \"who\" and tree.right.val == \"aux\":\n",
    "                self.left_modifier_generate(tree)\n",
    "\n",
    "        self.generate(tree.left)\n",
    "        self.generate(tree.right)\n",
    "\n",
    "    def delete_cc(self, tree):\n",
    "        if tree.val == \"cc\" and tree.left.val != \"but\":\n",
    "            self.delete_modifier(tree, tree.right)\n",
    "\n",
    "        if tree.is_tree:\n",
    "            self.delete_cc(tree.left)\n",
    "            self.delete_cc(tree.right)\n",
    "\n",
    "    def delete_modifier(self, tree, modifier):\n",
    "        tree.val = modifier.val\n",
    "        tree.mark = modifier.mark\n",
    "        tree.pos = modifier.pos\n",
    "        tree.id = modifier.id\n",
    "        \n",
    "        tree.is_tree = modifier.is_tree\n",
    "        tree.is_root = modifier.is_root\n",
    "\n",
    "        tree.left = modifier.left\n",
    "        tree.right = modifier.right\n",
    "\n",
    "        self.delete_cc(tree)\n",
    "        self.save_tree()\n",
    "\n",
    "    def delete_left_modifier(self, tree):\n",
    "        #print(\"Delet: \", tree.left.val)\n",
    "        group = self.removeGroup(tree)\n",
    "        if(not group):\n",
    "            self.delete_modifier(tree, tree.right)\n",
    "\n",
    "    def delete_right_modifier(self, tree):\n",
    "        #print(\"Delet: \", tree.right.val)\n",
    "        self.delete_modifier(tree, tree.left)\n",
    "\n",
    "    def rollback(self, tree, backup):\n",
    "        tree.val = backup.val\n",
    "        tree.left = deepcopy(backup.left)\n",
    "        tree.right = deepcopy(backup.right)\n",
    "        tree.mark = backup.mark\n",
    "        tree.pos = backup.pos\n",
    "        tree.id = backup.id\n",
    "        tree.is_tree = backup.is_tree\n",
    "        tree.is_root = backup.is_root\n",
    "\n",
    "    def symmetric_generate(self, tree):\n",
    "        self.right_modifier_generate(tree)\n",
    "        self.left_modifier_generate(tree)\n",
    "        #self.delete_cc(tree)\n",
    "\n",
    "    def right_modifier_generate(self, tree):\n",
    "        left = tree.left\n",
    "        right = tree.right\n",
    "        backup = deepcopy(tree)\n",
    "\n",
    "        self.delete_right_modifier(tree)\n",
    "        self.save_tree()\n",
    "        self.rollback(tree, backup)    \n",
    "        \n",
    "        self.generate(tree.left)\n",
    "        self.generate(tree.right)\n",
    "\n",
    "    def left_modifier_generate(self, tree):\n",
    "        left = tree.left\n",
    "        right = tree.right\n",
    "        backup = deepcopy(tree)\n",
    "\n",
    "        self.delete_left_modifier(tree)\n",
    "        self.save_tree()\n",
    "        self.rollback(tree, backup)   \n",
    "\n",
    "        self.generate(tree.left)\n",
    "        self.generate(tree.right)\n",
    "    \n",
    "    def return_last_leaf(self, tree):\n",
    "        max_id = 0\n",
    "        max_id_l = 0\n",
    "        max_id_r = 0\n",
    "\n",
    "        if tree.id != None:\n",
    "            max_id = int(tree.id)\n",
    "    \n",
    "        if tree.left.is_tree:\n",
    "            max_id_l = self.return_last_leaf(tree.left)\n",
    "        else:\n",
    "            max_id_l = tree.left.id\n",
    "\n",
    "        if tree.right.is_tree:\n",
    "            max_id_r = self.return_last_leaf(tree.right)\n",
    "        else:\n",
    "            max_id_r = tree.right.id\n",
    "\n",
    "        return max(max_id, max(max_id_l, max_id_r))\n",
    "\n",
    "    def return_first_leaf(self, tree):\n",
    "        min_id = 100\n",
    "        min_id_l = 100\n",
    "        min_id_r = 100\n",
    "\n",
    "        if tree.id != None:\n",
    "            min_id = int(tree.id)\n",
    "    \n",
    "        if tree.left.is_tree:\n",
    "            min_id_l = self.return_last_leaf(tree.left)\n",
    "        else:\n",
    "            min_id_l = tree.left.id\n",
    "\n",
    "        if tree.right.is_tree:\n",
    "            min_id_r = self.return_last_leaf(tree.right)\n",
    "        else:\n",
    "            min_id_r = tree.right.id\n",
    "\n",
    "        return min(min_id, min(min_id_l, min_id_r))\n",
    "\n",
    "    def add_modifier_sent(self, tree, modifier, direct=0): \n",
    "        sentence = deepcopy(self.sentence)\n",
    "        if direct == 0:\n",
    "            last_leaf = self.return_first_leaf(tree)\n",
    "            sentence.insert(last_leaf-1, modifier)\n",
    "        elif direct == 1:\n",
    "            last_leaf = self.return_last_leaf(tree)\n",
    "            sentence.insert(last_leaf, modifier)        \n",
    "\n",
    "        self.remove_adjcent_duplicate(sentence)\n",
    "        sentence = ' '.join(sentence)\n",
    "        sentence = sentence.replace(\"-\", \" \")\n",
    "        sentence = sentence.replace(\" 's\", \"'s\")\n",
    "\n",
    "        if abs(len(sentence) - len(self.hypothesis)) < 15:\n",
    "            re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', sentence, flags = re.I)\n",
    "            sentence = sentence.strip() \n",
    "            \n",
    "            if sentence.lower() == self.hypothesis.lower():\n",
    "                self.stop_critarion = True\n",
    "                self.sent_log.append((sentence, 0.0))\n",
    "                return\n",
    "                \n",
    "            similarity = inference_sts(sentence, self.hypothesis, dist=True)\n",
    "            self.sent_log.append((sentence, similarity))\n",
    "\n",
    "            if similarity < 0.5:\n",
    "                self.sent_log.append((sentence, similarity))\n",
    "                self.stop_critarion = True\n",
    "\n",
    "    def add_modifier_lexical(self, tree, modifier, head, word_id, direct=0):\n",
    "        if direct == 0:\n",
    "            generated = ' '. join([modifier, head])\n",
    "        else:\n",
    "            generated = ' '. join([head, modifier])\n",
    "        \n",
    "        sentence = deepcopy(self.sentence)\n",
    "        diff = 0\n",
    "        if word_id > len(sentence):\n",
    "            diff = word_id - len(sentence)\n",
    "\n",
    "        goal = word_id-1-diff\n",
    "        sentence[goal] = \"DEL\"\n",
    "        sentence[goal:goal] = generated.split(' ')\n",
    "\n",
    "        if abs(len(sentence) - len(self.hypothesis.split(' '))) < 7:\n",
    "            self.remove_adjcent_duplicate(sentence)\n",
    "            sentence = ' '.join(sentence)\n",
    "            sentence = sentence.replace(\"DEL \", \"\")\n",
    "            sentence = sentence.replace(\"DEL\", \"\")\n",
    "            sentence = sentence.replace(\"-\", \" \")\n",
    "            sentence = sentence.replace(\" 's\", \"'s\")\n",
    "            re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', sentence, flags = re.I)\n",
    "            sentence = sentence.strip()\n",
    "\n",
    "            if sentence.lower() == self.hypothesis.lower():\n",
    "                self.stop_critarion = True\n",
    "                self.sent_log.append((sentence, 0.0))\n",
    "                return\n",
    "            \n",
    "            similarity = inference_sts(sentence, self.hypothesis, dist=True)\n",
    "            self.sent_log.append((sentence, similarity))\n",
    "\n",
    "            if similarity < 0.5:\n",
    "                self.sent_log.append((sentence, similarity))\n",
    "                self.stop_critarion = True\n",
    "\n",
    "    def generate_default(self, tree):\n",
    "        VP_rel = {\n",
    "            \"aux\":1, \n",
    "            \"obj\":1, \n",
    "            \"obl\":1, \n",
    "            \"xcomp\":1, \n",
    "            \"ccomp\":1,\n",
    "            \"aux:pass\":1, \n",
    "            \"obl:tmod\":1, \n",
    "            \"obl:npmod\":1\n",
    "        }\n",
    "\n",
    "        VP_mod = {\n",
    "            \"advcl\":1, \n",
    "            \"xcomp\":1, \n",
    "            \"ccomp\":1,\n",
    "            \"obj\":1, \n",
    "            \"advmod\":1, \n",
    "            \"obl\":1, \n",
    "            \"obl:tmod\":1,\n",
    "            \"obl:nmod\":1, \n",
    "            \"parataxis\":1, \n",
    "            \"conj\":1\n",
    "        }\n",
    "\n",
    "        NP_rel = {\n",
    "            \"amod\":1,\n",
    "            \"compound\":1,\n",
    "            \"det\":1,\n",
    "            \"mark\":1,\n",
    "            \"nmod:poss\":1,\n",
    "            \"flat\":1,\n",
    "            \"acl:relcl\":1,\n",
    "            \"acl\":1,\n",
    "            \"nmod\":1\n",
    "        }\n",
    "\n",
    "        NP_mod = {\n",
    "            \"amod\":1,\n",
    "            \"compound\":1,\n",
    "            \"det\":1,\n",
    "            \"mark\":1,\n",
    "            \"nmod:poss\":1,\n",
    "            \"flat\":1,\n",
    "        }\n",
    "\n",
    "        if tree.pos is not None:\n",
    "            if (\"NN\" in tree.pos or \"JJ\" in tree.pos) and tree.mark == \"-\":\n",
    "                for rel in [\"amod\", \"compound\", \"det\", \"mark\", \"nmod:poss\", \"flat\", \"conj\", \"nummod\"]:\n",
    "                    if rel in self.kb:\n",
    "                        for phrase in self.kb[rel]:\n",
    "                            if phrase['head'] == tree.val:\n",
    "                                self.add_modifier_lexical(tree, phrase['mod'], tree.val, tree.id)\n",
    "                for rel in [\"amod\", \"acl:relcl\", \"compound\", \"acl\", \"nmod\"]:\n",
    "                    if rel in self.kb:\n",
    "                        for phrase in self.kb[rel]:\n",
    "                            if phrase['head'] == tree.val:\n",
    "                                self.add_modifier_lexical(tree, phrase['mod'], tree.val, tree.id, 1)\n",
    "                \n",
    "            elif \"VB\" in tree.pos and tree.mark == \"-\":\n",
    "                for rel in [\"advmod\"]:\n",
    "                    if rel in self.kb:\n",
    "                        for phrase in self.kb[rel]:\n",
    "                            self.add_modifier_lexical(tree, phrase['mod'], tree.val, tree.id)\n",
    "                            self.add_modifier_lexical(tree, phrase['mod'], tree.val, tree.id, 1)\n",
    "\n",
    "        elif VP_rel.get(tree.val, 0) and tree.mark == \"-\":\n",
    "            for rel in VP_mod:\n",
    "                if rel in self.kb:\n",
    "                    for phrase in self.kb[rel]:\n",
    "                        self.add_modifier_sent(tree, phrase['mod'], direct=1)\n",
    "\n",
    "        elif NP_rel.get(tree.val, 0) and tree.mark == \"-\":\n",
    "            for rel in NP_mod:\n",
    "                if rel in self.kb:\n",
    "                    for phrase in self.kb[rel]:\n",
    "                        self.add_modifier_sent(tree, phrase['mod'], direct=0)\n",
    "        \n",
    "        if VP_rel.get(tree.val, 0) and tree.right.val == \"watching\":\n",
    "            self.save_tree(tree=tree.left)\n",
    "        if tree.is_tree:\n",
    "            self.generate(tree.left)\n",
    "            self.generate(tree.right)  \n",
    "\n",
    "    def save_tree(self, tree=None):\n",
    "        if tree is None:\n",
    "            leaves = self.deptree.sorted_leaves().popkeys()\n",
    "            tree_copy = self.deptree.copy()\n",
    "        else:\n",
    "            leaves = tree.sorted_leaves().popkeys()\n",
    "            tree_copy = tree.copy()\n",
    "        \n",
    "        sentence = ' '.join([x[0] for x in leaves])\n",
    "        sentence = sentence.replace(\"-\", \" \")\n",
    "        if sentence.lower() == self.hypothesis.lower():\n",
    "            self.tree_log = []\n",
    "            self.stop_critarion = True\n",
    "            self.tree_log.append((tree_copy, sentence, 0.0))\n",
    "            return\n",
    "        \n",
    "        similarity = inference_sts(sentence, self.hypothesis, dist=True)\n",
    "        self.tree_log.append((tree_copy, sentence, similarity))\n",
    "\n",
    "        if similarity < 0.5:\n",
    "            self.tree_log = []\n",
    "            self.tree_log.append((tree_copy, sentence, similarity))\n",
    "            self.stop_critarion = True\n",
    "    \n",
    "    def remove_adjcent_duplicate(self, string):\n",
    "        to_remove = -1\n",
    "        for i in range(len(string)-1):\n",
    "            if string[i] == string[i+1]:\n",
    "                to_remove = i\n",
    "        if to_remove > -1:\n",
    "            del string[to_remove]\n",
    "\n",
    "    def search_dependency(self, deprel, tree):\n",
    "        if tree.val == deprel:\n",
    "            return True\n",
    "        else:\n",
    "            right = tree.right\n",
    "            left = tree.left\n",
    "\n",
    "            left_found = False\n",
    "            right_found = False\n",
    "\n",
    "            if right is not None:\n",
    "                right_found = self.search_dependency(deprel, right)\n",
    "\n",
    "            if left is not None:\n",
    "                left_found = self.search_dependency(deprel, left)\n",
    "\n",
    "            return left_found or right_found\n",
    "    \n",
    "    def Diff(self, li1, li2):\n",
    "        return (list(list(set(li1)-set(li2)) + list(set(li2)-set(li1))))    \n",
    "    \n",
    "    def preprocess(self, sentence):\n",
    "        preprocessed = sentence.replace(\".\", \"\").replace(\"!\", \"\").replace(\"?\", \"\")\n",
    "        preprocessed = preprocessed.replace(\"can't\", \"can not\")\n",
    "        preprocessed = preprocessed.replace(\"couldn't\", \"could not\")\n",
    "        preprocessed = preprocessed.replace(\"don't\", \"do not\")\n",
    "        preprocessed = preprocessed.replace(\"doesn't\", \"does not\")\n",
    "        preprocessed = preprocessed.replace(\"isn't\", \"is not\")\n",
    "        preprocessed = preprocessed.replace(\"won't\", \"will not\")\n",
    "        preprocessed = preprocessed.replace(\"wasn't\", \"was not\")\n",
    "        preprocessed = preprocessed.replace(\"weren't\", \"were not\")\n",
    "        preprocessed = preprocessed.replace(\"didn't\", \"did not\")\n",
    "        preprocessed = preprocessed.replace(\"aren't\", \"are not\")\n",
    "        preprocessed = preprocessed.replace(\"it's\", \"it is\")\n",
    "        preprocessed = preprocessed.replace(\"wouldn't\", \"would not\")\n",
    "        preprocessed = preprocessed.replace(\"There's\", \"There is\")\n",
    "        return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifier_relation = {\n",
    "    \"NN\": [\"amod\", \"nmod\", \"acl:relcl\", \"fixed\", \"compound\", \"det\", \"nmod:poss\", \"conj\", \"nummod\"],\n",
    "    \"VB\": [\"advmod\", \"acl\", \"obl\", \"xcomp\", \"advcl\", \"obl:tmod\", \"parataxis\", \"obj\",\"ccomp\"]\n",
    "}\n",
    "\n",
    "def down_right(tree):\n",
    "    if(tree.right == None):\n",
    "        return tree\n",
    "    return down_right(tree.right)\n",
    "\n",
    "def down_left(tree):\n",
    "    if(tree.left == None):\n",
    "        return tree\n",
    "    return down_left(tree.left)\n",
    "\n",
    "def collect_modifiers(tree, sent_set, mod_type=\"NN\"):\n",
    "    leaves = []\n",
    "    if tree.is_tree:\n",
    "        if tree.val in [\"mark\", \"case\", \"compound\", \"flat\", \"nmod\"]:\n",
    "            leaves.append(\n",
    "                (list(tree.right.sorted_leaves().popkeys()),\n",
    "                down_right(tree.left).val)\n",
    "            )\n",
    "        if tree.val in modifier_relation[mod_type]:\n",
    "            leaves.append(\n",
    "                (list(tree.left.sorted_leaves().popkeys()),\n",
    "                down_right(tree.right).val)\n",
    "            )\n",
    "\n",
    "        for leave in leaves:\n",
    "            if len(leave) > 0 and len(leave) < 10:\n",
    "                head = leave[1]\n",
    "                modifier = ' '.join([x[0] for x in leave[0]])\n",
    "                if tree.val in sent_set:\n",
    "                    sent_set[tree.val].append({'head': head,'mod': modifier})\n",
    "                else:\n",
    "                    sent_set[tree.val] = [{'head': head,'mod': modifier}]\n",
    "        \n",
    "        collect_modifiers(tree.left, sent_set, mod_type)\n",
    "        collect_modifiers(tree.right, sent_set, mod_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      "\n",
      "Init Premise: A man with a helmet painted red is riding a blue motorcycle down the road\n",
      "\n",
      "Hypothesis: A man with a helmet is riding a blue motorcycle down the road\n",
      "{   'amod': [{'head': 'motorcycle', 'mod': 'blue'}],\n",
      "    'case': [   {'head': 'with', 'mod': 'a helmet'},\n",
      "                {'head': 'down', 'mod': 'the road'},\n",
      "                {'head': 'with', 'mod': 'a helmet'},\n",
      "                {'head': 'down', 'mod': 'the road'}],\n",
      "    'det': [   {'head': 'helmet', 'mod': 'a'},\n",
      "               {'head': 'man', 'mod': 'A'},\n",
      "               {'head': 'road', 'mod': 'the'},\n",
      "               {'head': 'motorcycle', 'mod': 'a'}],\n",
      "    'nmod': [   {'head': 'helmet', 'mod': 'A man'},\n",
      "                {'head': 'man', 'mod': 'with a helmet'},\n",
      "                {'head': 'helmet', 'mod': 'A man'}],\n",
      "    'obj': [{'head': 'riding', 'mod': 'a blue motorcycle'}],\n",
      "    'obl': [{'head': 'riding', 'mod': 'down the road'}]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A man with a helmet is riding a blue motorcycle down the road', 0.0)\n",
      "\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "up = [\"A man with a helmet painted red is riding a blue motorcycle down the road\"]\n",
    "up_h = [\"A man with a helmet is riding a blue motorcycle down the road\"]\n",
    "\n",
    "annotations = []\n",
    "phrasalGenerator = PhrasalGenerator()\n",
    "pipeline = PolarizationPipeline(verbose=0)\n",
    "for i in range(len(up)):\n",
    "    premise = up[i]\n",
    "    hypothesis = up_h[i]\n",
    "    premise = phrasalGenerator.preprocess(premise)\n",
    "    hypothesis = phrasalGenerator.preprocess(hypothesis)\n",
    "\n",
    "    tokenized = tokenizer(premise).sentences[0].words\n",
    "    tokens = [tok.text for tok in tokenized]\n",
    "\n",
    "    print(\"\\n====================================\")\n",
    "    print(\"\\nInit Premise: \" + premise)\n",
    "    print(\"\\nHypothesis: \" + hypothesis)\n",
    "\n",
    "    h_parsed, replaced = dependency_parse(hypothesis, parser=\"stanza\")\n",
    "    h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "    pipeline.modify_replacement(h_tree, replaced)\n",
    "    phrases = {} \n",
    "    collect_modifiers(h_tree, phrases, mod_type=\"NN\")\n",
    "    collect_modifiers(h_tree, phrases, mod_type=\"VB\")\n",
    "    annotation = pipeline.single_polarization(premise)\n",
    "    \n",
    "    phrasalGenerator.kb = phrases\n",
    "    phrasalGenerator.hypothesis = hypothesis.replace(',', '')\n",
    "    pp.pprint(phrasalGenerator.kb)\n",
    "    \n",
    "    polarized = pipeline.postprocess(annotation['polarized_tree'], {})\n",
    "    btreeViz = Tree.fromstring(polarized.replace('[', '(').replace(']', ')'))\n",
    "    jupyter_draw_nltk_tree(btreeViz) \n",
    "\n",
    "    polarized = pipeline.postprocess(h_tree, {})\n",
    "    btreeViz = Tree.fromstring(polarized.replace('[', '(').replace(']', ')'))\n",
    "    jupyter_draw_nltk_tree(btreeViz)\n",
    "    \n",
    "    phrasalGenerator.deptree_generate(\n",
    "        annotation['polarized_tree'], \n",
    "        annotation['annotated'], tokens)\n",
    "\n",
    "    for gen_tree in phrasalGenerator.tree_log:\n",
    "        #leaves = gen_tree[0].sorted_leaves().popkeys()\n",
    "        #sentence = ' '.join([x[0] for x in leaves])\n",
    "        print((gen_tree[1], gen_tree[2]))\n",
    "\n",
    "    print(*phrasalGenerator.sent_log, sep=\"\\n\")\n",
    "    print(phrasalGenerator.stop_critarion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lexical Monotonicity Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import wordnet\n",
    "import importlib\n",
    "importlib.reload(wordnet)\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemmatize(word):\n",
    "    doc = nlp(word)\n",
    "    for token in doc:\n",
    "        return token.lemma_\n",
    "    \n",
    "class LexicalGenerator:\n",
    "    def __init__(self):\n",
    "        self.deptree = None\n",
    "        self.premise = \"\"\n",
    "        self.hypothesis = \"\"\n",
    "        self.tree_log = []\n",
    "        self.ant_ht = {}\n",
    "        self.sentence_base = set()\n",
    "        self.anti_tree_log = []\n",
    "        self.polar_log = []\n",
    "        self.replacement_log = []\n",
    "        self.stop_critarion = False\n",
    "        self.key_tokens = [\n",
    "            'NN','NNS','NNP','NNPS','VBD',\n",
    "            'VBG','VBN','VBZ','VB',\"JJ\"]\n",
    "        self.absolute = False\n",
    "        self.propers = [\"someone\", \"something\", \"somewhere\"]\n",
    "        self.memory = {}\n",
    "        self.nondetermine = False\n",
    "        self.quantifiers = {}\n",
    "        self.lemmatizer = WordNetLemmatizer() \n",
    "        with open('quantifier.json', 'r') as quants:\n",
    "            quantifier_data = json.load(quants)\n",
    "            for quantifier in quantifier_data:\n",
    "                self.quantifiers[quantifier['word']] = quantifier\n",
    "\n",
    "    def absolute_generate(self, src, dest):\n",
    "        #mono = list(zip(*annotation))[2]\n",
    "        self.nondetermine = False\n",
    "        hyper, hypo, syn, ant, _ = wordnet.get_word_sets(\n",
    "            lemmatize(src))\n",
    "        \n",
    "        self.ant_ht = {**self.ant_ht, **ant}\n",
    "        _, _, syn_multi, _, _ = wordnet.get_word_sets(src)\n",
    "        syn_full = {**syn, **syn_multi} \n",
    "        if src == \"boy\" or src == \"girl\":\n",
    "            if \"young\" in syn_full:\n",
    "                syn_full[\"young\"] = 0\n",
    "        self.memory[src] = (hyper, hypo, syn_full, ant)\n",
    "        if src == \"boy\":\n",
    "            ant['chick'] = 1 \n",
    "        if src == \"running\":\n",
    "            ant['stand'] = 1\n",
    "\n",
    "        if lemmatize(dest) in ant:\n",
    "            self.nondetermine = True\n",
    "            return False\n",
    "\n",
    "        #if mono[id] == \"+\":\n",
    "\n",
    "\n",
    "    def deptree_generate(self, tree):\n",
    "        self.replacement_log = []\n",
    "        self.nondetermine = False\n",
    "        self.sentence_base = set()\n",
    "        self.tree_log = []\n",
    "        self.anti_tree_log = []\n",
    "        self.stop_critarion = False\n",
    "        self.deptree = tree.copy()\n",
    "        self.generate(self.deptree)\n",
    "\n",
    "    def generate(self, tree):\n",
    "        if tree is None or self.stop_critarion:\n",
    "            return\n",
    "        if tree.pos is not None and not tree.val in self.hypothesis: \n",
    "            backup = copy(tree.val)\n",
    "            if tree.pos == \"NNP\" and tree.mark == \"+\":\n",
    "                for word in self.propers:\n",
    "                    if word in self.hypothesis_tokens:\n",
    "                        tree.val = word\n",
    "                        self.save_tree()\n",
    "                        self.replacement_log.append(\n",
    "                            \"{} => {}\".format(backup, word))\n",
    "                        tree.val = backup\n",
    "\n",
    "            if tree.pos in self.key_tokens:\n",
    "                if tree.val in self.memory:\n",
    "                    hyper, hypo, syn_full, ant = self.memory[tree.val]\n",
    "                    self.ant_ht = {**self.ant_ht, **ant}\n",
    "                else:\n",
    "                    hyper, hypo, syn, ant, _ = wordnet.get_word_sets(\n",
    "                        self.lemmatizer.lemmatize(tree.val))\n",
    "                    self.ant_ht = {**self.ant_ht, **ant}\n",
    "                    _, _, syn_multi, _, _ = wordnet.get_word_sets(tree.val)\n",
    "                    syn_full = {**syn, **syn_multi} \n",
    "                    if tree.val == \"boy\" or tree.val == \"girl\":\n",
    "                        if \"young\" in syn_full:\n",
    "                            syn_full[\"young\"] = 0\n",
    "                    self.memory[tree.val] = (hyper, hypo, syn_full, ant)\n",
    "\n",
    "                if tree.val == \"boy\":\n",
    "                    self.ant_ht['chick'] = 1    \n",
    "\n",
    "                for lex in syn_full.keys():\n",
    "                    lex_ls = lex.split(' ')\n",
    "                    is_eq = False\n",
    "                    for key in lex_ls:\n",
    "                        if ant.get(key, 0):\n",
    "                            self.ant_ht[key] = 1\n",
    "                            if self.absolute:\n",
    "                                self.nondetermine = True\n",
    "                                return False\n",
    "                        if key in self.ant_ht:\n",
    "                            if self.absolute:\n",
    "                                self.nondetermine = True\n",
    "                                return False\n",
    "                            break\n",
    "                        if not ign_words.get(key,0):\n",
    "                            is_eq = True\n",
    "                    if is_eq:\n",
    "                        tree.val = lex\n",
    "                        self.save_tree()\n",
    "                        self.replacement_log.append(\n",
    "                            \"{} => {}\".format(backup, lex))\n",
    "                tree.val = backup\n",
    "\n",
    "                if tree.mark == \"+\":\n",
    "                    if tree.val == \"pianist\":\n",
    "                        tree.val = \"person\"\n",
    "                        self.save_tree()\n",
    "                        self.replacement_log.append(\n",
    "                            \"{} => {}\".format(backup, \"person\"))          \n",
    "                    for lex in hyper.keys():\n",
    "                        \n",
    "                        lex_ls = lex.split(' ')\n",
    "                        for key in lex_ls:\n",
    "                            is_eq = False\n",
    "                            if not ign_words.get(key,0):\n",
    "                                for tok in self.hypothesis_tokens:\n",
    "                                    #print(self.ant_ht)\n",
    "                                    if ant.get(key, 0):\n",
    "                                        self.ant_ht[key] = 1\n",
    "                                        if self.absolute:\n",
    "                                            self.nondetermine = True\n",
    "                                            return False\n",
    "                                        break\n",
    "                                    if key in self.ant_ht:\n",
    "                                        if self.absolute:\n",
    "                                            self.nondetermine = True\n",
    "                                            return False\n",
    "                                        break\n",
    "                                    if tok in key:\n",
    "                                        is_eq = True\n",
    "                                if is_eq:\n",
    "                                    tree.val = lex\n",
    "                                    self.save_tree()\n",
    "                                    self.replacement_log.append(\n",
    "                                        \"{} => {}\".format(backup, lex))\n",
    "                    tree.val = backup\n",
    "\n",
    "                if tree.mark == \"-\":\n",
    "                    for lex in hypo.keys():\n",
    "                        lex_ls = lex.split(' ')\n",
    "                        for key in lex_ls:\n",
    "                            #print(key)\n",
    "                            #print(self.hypothesis_tokens)\n",
    "                            if not ign_words.get(key,0):\n",
    "                                for tok in self.hypothesis_tokens:\n",
    "                                    if tok in key or key in tok:\n",
    "                                        tree.val = tok\n",
    "                                        self.save_tree()\n",
    "                                        self.replacement_log.append(\n",
    "                                            \"{} => {}\".format(backup, tok))\n",
    "                    tree.val = backup\n",
    "            \n",
    "        elif tree.val == \"det\":\n",
    "            backup = tree.left.val\n",
    "            backup_mark = tree.right.mark\n",
    "            kb = self.quantifiers.get(tree.left.val.lower(), {})\n",
    "            if len(kb) > 0:\n",
    "\n",
    "                for word in kb[\"=\"]:\n",
    "                    tree.left.val = word\n",
    "                    detType = det_type(tree.left.val)\n",
    "                    if detType is None:\n",
    "                        detType = \"det:exist\"\n",
    "                    tree.left.mark = det_mark[detType]\n",
    "                    self.save_tree()\n",
    "                    self.replacement_log.append(\n",
    "                        \"{} => {}\".format(backup, word))\n",
    "                tree.left.val = backup\n",
    "                tree.left.mark = backup_mark\n",
    "\n",
    "                if tree.left.mark == \"+\":\n",
    "                    for word in kb[\"<\"]:\n",
    "                        if word in self.hypothesis:\n",
    "                            tree.left.val = word\n",
    "                            detType = det_type(tree.left.val)\n",
    "                            if detType is None:\n",
    "                                detType = \"det:exist\"\n",
    "                            tree.left.mark = det_mark[detType]\n",
    "                            self.save_tree()\n",
    "                            self.replacement_log.append(\n",
    "                                \"{} => {}\".format(backup, word))\n",
    "                    tree.left.val = backup\n",
    "                    tree.left.mark = backup_mark\n",
    "                \n",
    "                if tree.left.mark == \"-\":\n",
    "                    for word in kb[\">\"]:\n",
    "                        if word in self.hypothesis:\n",
    "                            tree.left.val = word\n",
    "                            if detType is None:\n",
    "                                detType = \"det:exist\"\n",
    "                            tree.left.mark = det_mark[detType]\n",
    "                            self.save_tree()\n",
    "                            self.replacement_log.append(\n",
    "                                \"{} => {}\".format(backup, word))\n",
    "                    tree.left.val = backup\n",
    "                    tree.left.mark = backup_mark\n",
    "\n",
    "        elif tree.val == \"nummod\":\n",
    "            backup = tree.left.val\n",
    "            if tree.left.mark != \"-\":\n",
    "                tree.left.val = \"some\"\n",
    "                self.save_tree()\n",
    "                self.replacement_log.append(\n",
    "                    \"{} => {}\".format(backup, \"some\"))\n",
    "                tree.left.val = backup\n",
    "        \n",
    "        if tree.left != \"N\":\n",
    "            self.generate(tree.left)\n",
    "        if tree.right != \"N\":\n",
    "            self.generate(tree.right)\n",
    "\n",
    "    def save_tree(self, entail=True):\n",
    "        leaves = self.deptree.sorted_leaves().popkeys()\n",
    "        tree_copy = self.deptree.copy()\n",
    "     \n",
    "        sentence = ' '.join([x[0] for x in leaves])\n",
    "        \n",
    "        if not sentence in self.sentence_base:\n",
    "            self.sentence_base.add(sentence)\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "        if sentence.lower() == self.hypothesis.lower():\n",
    "            self.stop_critarion = True\n",
    "            if entail:\n",
    "                self.tree_log = []\n",
    "                self.tree_log.append((tree_copy, sentence, 0.0))\n",
    "            else:\n",
    "                self.anti_tree_log = []\n",
    "                self.anti_tree_log.append((tree_copy, sentence, 0.0))\n",
    "            return\n",
    "        \n",
    "        similarity = inference_sts(sentence, self.hypothesis, dist=True)\n",
    "        #print(sentence, similarity)\n",
    "        if entail:\n",
    "            self.tree_log.append((tree_copy, sentence, similarity))\n",
    "        else:\n",
    "            self.anti_tree_log.append((tree_copy, sentence, similarity))\n",
    "        if similarity < 0.5:\n",
    "            self.stop_critarion = True\n",
    "            if entail:\n",
    "                self.tree_log = []\n",
    "                self.tree_log.append((tree_copy, sentence, similarity))\n",
    "            else:\n",
    "                self.anti_tree_log = []\n",
    "                self.anti_tree_log.append((tree_copy, sentence, similarity))\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nsubj  [det  [DT  A]  [NN  boy]]  [aux  [VBZ  is]  [obl  [case  [IN  in]  [det  [DT  a]  [NN  seat]]]  [VBG  sitting]]]]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('some boy is sitting in a seat', tensor(19.7621))\n('an boy is sitting in a seat', tensor(19.3686))\n('one boy is sitting in a seat', tensor(20.6456))\n('A male child is sitting in a seat', tensor(20.7740))\n('A boy is sitting in a seat', tensor(19.3240))\n('A son is sitting in a seat', tensor(21.2104))\n('A brother is sitting in a seat', tensor(23.4637))\n('A child is sitting in a seat', tensor(13.9254))\n('A men is sitting in a seat', tensor(19.3145))\n('A lad is sitting in a seat', tensor(19.3163))\n('A camp is sitting in a seat', tensor(16.5167))\n('A tomgirl is sitting in a seat', tensor(15.9334))\n('A boyloving is sitting in a seat', tensor(21.2955))\n('A shopboy is sitting in a seat', tensor(21.0751))\n('A caveboy is sitting in a seat', tensor(22.0299))\n('A camboy is sitting in a seat', tensor(22.2618))\n('A boy is sitting in some seat', tensor(19.0991))\n('A boy is sitting in an seat', tensor(19.2870))\n('A boy is sitting in one seat', tensor(20.1228))\n['A => some', 'A => an', 'A => one', 'boy => male child', 'boy => boy', 'boy => son', 'boy => brother', 'boy => child', 'boy => men', 'boy => lad', 'boy => camp', 'boy => tomgirl', 'boy => boyloving', 'boy => shopboy', 'boy => caveboy', 'boy => camboy', 'a => some', 'a => an', 'a => one']\nFalse\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "sentences = [\"A boy is sitting in a seat\"]\n",
    "hypotheses = [\"A girl is sitting in a seat\"]\n",
    "\n",
    "pipeline = PolarizationPipeline(verbose=0)\n",
    "lexicalGenerator = LexicalGenerator()\n",
    "phrasalGenerator = PhrasalGenerator()\n",
    "\n",
    "for premise, hypothesis in zip(sentences, hypotheses):\n",
    "    premise = phrasalGenerator.preprocess(premise)\n",
    "    hypothesis = phrasalGenerator.preprocess(hypothesis)\n",
    "\n",
    "    h_parsed, replaced = dependency_parse(hypothesis, parser=\"stanza\")\n",
    "    #h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "    #pipeline.modify_replacement(h_tree, replaced)\n",
    "    key_tokens = set()\n",
    "    for word in h_parsed[2]:\n",
    "        pos = h_parsed[2][word][1]\n",
    "        if 'NN' in pos or 'JJ' in pos or 'VB' in pos:\n",
    "            if(not ign_words.get(h_parsed[2][word][0],0)):\n",
    "                key_tokens.add(h_parsed[2][word][0])\n",
    "\n",
    "    #print(\"\\n====================================\")\n",
    "    #print(\"\\nInit Premise: \" + premise)\n",
    "    #print(\"\\nHypothesis: \" + hypothesis)\n",
    "\n",
    "    #tokenized = tokenizer(hypothesis).sentences[0].words\n",
    "    #tokens = {} \n",
    "    #lemmatizer = WordNetLemmatizer() \n",
    "    #for tok in tokenized:\n",
    "    #    tokens[lemmatizer.lemmatize(tok.text)] = tok.text\n",
    "    lexicalGenerator.hypothesis_tokens = key_tokens\n",
    "\n",
    "    annotation = pipeline.single_polarization(premise)\n",
    "    polarized = pipeline.postprocess(annotation['polarized_tree'], {})\n",
    "    print(polarized)\n",
    "    btreeViz = Tree.fromstring(polarized.replace('[', '(').replace(']', ')'))\n",
    "    jupyter_draw_nltk_tree(btreeViz) \n",
    "\n",
    "    lexicalGenerator.hypothesis = hypothesis.replace(',', '')\n",
    "    lexicalGenerator.deptree_generate(annotation['polarized_tree'])\n",
    "    \n",
    "    for gen_tree in lexicalGenerator.tree_log:\n",
    "        print((gen_tree[1], gen_tree[2]))\n",
    "    for anti_tree in lexicalGenerator.anti_tree_log:\n",
    "        print((anti_tree[1], anti_tree[2]))\n",
    "\n",
    "    print(lexicalGenerator.replacement_log)\n",
    "    print(lexicalGenerator.stop_critarion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458\n",
      "1738\n"
     ]
    }
   ],
   "source": [
    "MED_upward = []\n",
    "MED_upward_hypo = []\n",
    "MED_downward = []\n",
    "MED_downward_hypo = []\n",
    "\n",
    "with open(\"../data/MED/upward.txt\") as upward_med:\n",
    "    lines = upward_med.readlines()\n",
    "    for i in range(len(lines) // 4):\n",
    "        MED_upward.append(lines[i*4+1])\n",
    "        MED_upward_hypo.append(lines[i*4+2])\n",
    "\n",
    "with open(\"../data/MED/downward.txt\") as donward_med:\n",
    "    lines = donward_med.readlines()\n",
    "    for i in range(len(lines) // 4):\n",
    "        MED_downward.append(lines[i*4+1])\n",
    "        MED_downward_hypo.append(lines[i*4+2])\n",
    "\n",
    "print(len(MED_upward))\n",
    "print(len(MED_downward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Syntactic Variational Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import controller\n",
    "import importlib\n",
    "#importlib.reload(controller)\n",
    "\n",
    "class SyntacticVariator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chunker = controller.Chunker()\n",
    "        self.paraphraseTokenizer = paraphraseTokenizer\n",
    "        self.paraphraseModel = paraphraseModel\n",
    "        self.replacement_log = []\n",
    "        self.knowledge = [\n",
    "            (\"A man with a helmet\", \"A motorcyclist\"),\n",
    "            (\"A man, a woman and two girls\", \"A group of people\"),\n",
    "            (\"A man, a woman and two girls\", \"Four people\"),\n",
    "        ]\n",
    "\n",
    "    def chunking(self, tree):\n",
    "        return self.chunker.get_chunks_byDepTree(tree)\n",
    "\n",
    "    def build_pairs(self, chunks1, chunks2):\n",
    "        chunk_pairs = []\n",
    "        for chunk1 in chunks1:\n",
    "            for chunk2 in chunks2:\n",
    "                if len(set(chunk1.split(' ')).intersection(chunk2.split(' '))) > 0:\n",
    "                     chunk_pairs.append((chunk1, chunk2))\n",
    "\n",
    "        return chunk_pairs\n",
    "\n",
    "    def inference_mrpc(self, seq1, seq2):\n",
    "        paraphrase = paraphraseTokenizer.encode_plus(\n",
    "            seq1, seq2, return_tensors=\"pt\")\n",
    "        paraphrase.to('cuda')\n",
    "        logits = paraphraseModel(**paraphrase)[0]\n",
    "        paraphrase_results = torch.softmax(logits, dim=1).tolist()[0]\n",
    "        return paraphrase_results[1]\n",
    "\n",
    "    def phrase_alignment(self, chunk_pairs):\n",
    "        alignments = []\n",
    "        for pair in chunk_pairs:\n",
    "            score = self.inference_mrpc(pair[0], pair[1])\n",
    "            #print(pair, score)\n",
    "            if score > 0.80:\n",
    "                #if len(set(pair[0].split(' ')) - set(pair[1].split(' '))) > 1:\n",
    "                alignments.append(pair)\n",
    "\n",
    "        return alignments\n",
    "\n",
    "    def check_passact(self, ie1, ie2):\n",
    "        for verb in ie1:\n",
    "            if 'ARG1' in verb['tags'][0] or 'ARG1' in verb['tags'][1]:\n",
    "                return True\n",
    "\n",
    "        for verb in ie2:\n",
    "            if 'ARG1' in verb['tags'][0] or 'ARG1' in verb['tags'][1]:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def variate(self, P, H, p_tree, h_tree, sent=False):\n",
    "        self.replacement_log = []\n",
    "        p_chunks = self.chunking(p_tree)\n",
    "        h_chunks = self.chunking(h_tree)\n",
    "\n",
    "        ie_pred_p = ie_extractor.predict(P)['verbs']\n",
    "        ie_pred_h = ie_extractor.predict(H)['verbs']\n",
    "\n",
    "        if self.check_passact(ie_pred_p, ie_pred_h):\n",
    "            sent = True\n",
    "\n",
    "        if sent:\n",
    "            p_chunks.append(P)\n",
    "            h_chunks.append(H)\n",
    "\n",
    "        #for verb in ie_pred_p:\n",
    "        #    if \"ARG\" in verb['description']:\n",
    "        #        p_chunks.append(fix_info(verb['description'])[0].strip())\n",
    "        #        p_chunks.append(verb['verb'] + ' '+ fix_info(verb['description'])[2].strip())\n",
    "\n",
    "        #for verb in ie_pred_h:\n",
    "        #    if \"ARG\" in verb['description']:\n",
    "        #        h_chunks.append(fix_info(verb['description'])[0].strip())\n",
    "        #        h_chunks.append(verb['verb'] + ' '+ fix_info(verb['description'])[2].strip())\n",
    "\n",
    "        chunk_pairs = self.build_pairs(p_chunks, h_chunks)\n",
    "        alignments = self.phrase_alignment(chunk_pairs)\n",
    "\n",
    "        for relation in self.knowledge:\n",
    "            if relation[0] in P and relation[1] in H:\n",
    "                alignments.append((relation[0], relation[1]))\n",
    "\n",
    "        variates = set()\n",
    "        for align in alignments:\n",
    "            #alignList1 = align[1].split(' ')\n",
    "            #if(alignList1[0] == \"Somebody\"):\n",
    "            #    alignList2 = align[0].split(' ')\n",
    "            #    var_sentence = P.replace(' '.join(alignList2[1:]), ' '.join(alignList1[1:]))\n",
    "            #    self.replacement_log.append(\n",
    "            #            \"{} => {}\".format(' '.join(alignList2[1:]), ' '.join(alignList1[1:])))\n",
    "            #else:\n",
    "            var_sentence = P.replace(align[0], align[1])\n",
    "            self.replacement_log.append(\n",
    "                        \"{} => {}\".format(align[0], align[1]))\n",
    "            \n",
    "            variates.add(var_sentence)\n",
    "\n",
    "        return variates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Admin\\AppData\\Local\\Temp\\tmpt2p__kna\\config.json as plain json\n"
     ]
    }
   ],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.structured_prediction\n",
    "\n",
    "ie_extractor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{2: [['det', 1, 2], ['nmod', 5, 2]],\n",
       " 9: [['nsubj', 2, 9], ['aux', 8, 9], ['obj', 12, 9], ['obl', 15, 9]],\n",
       " 5: [['case', 3, 5], ['det', 4, 5], ['acl', 6, 5]],\n",
       " 6: [['advmod', 7, 6]],\n",
       " 'root': [['root', 9, 'root']],\n",
       " 12: [['det', 10, 12], ['amod', 11, 12]],\n",
       " 15: [['case', 13, 15], ['det', 14, 15]]}"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "h_parsed, replaced = dependency_parse(\"A man with a helmet painted red is riding a blue motorcycle down the road\", parser=\"stanza\")\n",
    "h_parsed[2][\"root\"] = ('0', 'ROOT')\n",
    "#h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "#pipeline.modify_replacement(h_tree, replaced)\n",
    "\n",
    "vertices = {}\n",
    "knowledge = []\n",
    "edges = {}\n",
    "dep_pair = {}\n",
    "for rel in h_parsed[0]:\n",
    "    dep_pair[rel[1]] = (rel[2], rel[0])\n",
    "conj_v = []\n",
    "for rel in h_parsed[0]:\n",
    "    if rel[0] == \"conj-vb\":\n",
    "        vid = rel[1]\n",
    "        rel[0] = dep_pair[rel[1]][1]\n",
    "        rel[1] = dep_pair[rel[1]][0]\n",
    "        rel[2] = vid\n",
    "\n",
    "    head = rel[2] #h_parsed[2][rel[2]][0] + '_' + str(rel[2])\n",
    "    mod = rel[1]  #h_parsed[2][rel[1]][0] + '_' + str(rel[1])\n",
    "\n",
    "    edges[head] = mod\n",
    "\n",
    "    if head in vertices:\n",
    "        vertices[head].append(rel)\n",
    "    else: \n",
    "        vertices[head] = [rel]\n",
    "        \n",
    "vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('A man with a helmet painted red', 'riding', 'a blue motorcycle')\n('A man with a helmet painted red', 'riding', 'down the road')\n"
     ]
    }
   ],
   "source": [
    "from pqdict import pqdict\n",
    "\n",
    "def extract_entity(nodes):\n",
    "    word_queue = pqdict([])\n",
    "    for node in nodes:\n",
    "        word_queue[node[1]] = node[1]\n",
    "        subnodes = vertices.get(node[1], [])\n",
    "        subwords = extract_entity(subnodes)\n",
    "        for word in subwords:\n",
    "            word_queue[word] = subwords[word]\n",
    "    return word_queue\n",
    "\n",
    "\n",
    "for rel in h_parsed[0]:\n",
    "    if rel[0] in ['nsubj']:\n",
    "        subj_nodes = vertices.get(rel[1], []) + [rel]\n",
    "        word_queue = extract_entity(subj_nodes)\n",
    "        keys = list(word_queue.popkeys())\n",
    "        subj = ' '.join([h_parsed[2][k][0] for k in keys])\n",
    "        objs = []\n",
    "        for node in vertices[rel[2]]:\n",
    "            if node[0] in ['obj', 'xcomp', 'obl', 'case']:\n",
    "                subnodes = vertices.get(node[1], []) + [node]\n",
    "                word_queue = extract_entity(subnodes)\n",
    "                keys = list(word_queue.popkeys())\n",
    "                objs.append(' '.join([h_parsed[2][k][0] for k in keys]))\n",
    "        knowledges = []\n",
    "        relation = h_parsed[2][rel[2]][0]\n",
    "        for obj in objs:\n",
    "            print((subj, relation, obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a big wave is being ridden by a surfer tensor(5.9572)\n",
      "is riding tensor(26.9629)\n",
      "A big wave is riding tensor(13.5435)\n",
      "is riding a big wave tensor(12.4002)\n",
      "The surfer is riding a big wave tensor(0.)\n",
      "A big wave is riding by a surfer tensor(6.3803)\n",
      "A big wave is riding a big wave tensor(11.9246)\n",
      "A big wave The surfer is riding a big wave tensor(3.4807)\n",
      "A big wave is being ridden The surfer tensor(6.6682)\n",
      "['is being ridden => is riding', 'is being ridden by a surfer => is riding a big wave', 'is being ridden by a surfer => is riding', 'is being ridden by a surfer => The surfer is riding a big wave', 'by a surfer => The surfer', 'A big wave => a big wave', 'A big wave is being ridden by a surfer => is riding a big wave', 'A big wave is being ridden by a surfer => is riding', 'A big wave is being ridden by a surfer => The surfer is riding a big wave']\n"
     ]
    }
   ],
   "source": [
    "premise = \"A big wave is being ridden by a surfer\"\n",
    "hypothesis = \"The surfer is riding a big wave\"\n",
    "\n",
    "pipeline = PolarizationPipeline()\n",
    "syntacticVariator = SyntacticVariator()\n",
    "\n",
    "h_parsed, replaced = dependency_parse(hypothesis, parser=\"stanza\")\n",
    "h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "pipeline.modify_replacement(h_tree, replaced)\n",
    "annotation = pipeline.single_polarization(premise)\n",
    "\n",
    "variates = syntacticVariator.variate(premise, hypothesis, annotation['polarized_tree'],  h_tree)\n",
    "for v in variates:\n",
    "    similarity = inference_sts(v, hypothesis, dist=True)\n",
    "    print(v, similarity)\n",
    "print(syntacticVariator.replacement_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. A* Inference Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pqdict import pqdict\n",
    "\n",
    "class AStarPlanner:\n",
    "    def __init__(self, verbose=0):    \n",
    "        self.closed = []                  \n",
    "        self.entailments = set()\n",
    "        self.contradictions = set()\n",
    "        self.controller = controller.Controller()\n",
    "        self.hypothesis = \"\"\n",
    "        self.h_tree = None\n",
    "        self.verbose = verbose\n",
    "        self.pipeline = PolarizationPipeline()\n",
    "        self.phrasalGenerator = PhrasalGenerator()\n",
    "        self.lexicalGenerator = LexicalGenerator()\n",
    "        self.syntacticVariator = SyntacticVariator() \n",
    "\n",
    "    def hypothesis_kb(self):\n",
    "        self.hypothesis = self.phrasalGenerator.preprocess(self.hypothesis).replace('\\n', '')\n",
    "        h_parsed, replaced = dependency_parse(self.hypothesis, parser=\"stanza\")\n",
    "        h_tree, _ = self.pipeline.run_binarization(h_parsed, self.hypothesis, {})\n",
    "        self.pipeline.modify_replacement(h_tree, replaced)\n",
    "        phrases = {} \n",
    "        collect_modifiers(h_tree, phrases, mod_type=\"NN\")\n",
    "        collect_modifiers(h_tree, phrases, mod_type=\"VB\")\n",
    "        self.phrasalGenerator.kb = phrases\n",
    "        key_tokens = set()\n",
    "        for word in h_parsed[2]:\n",
    "            pos = h_parsed[2][word][1]\n",
    "            if 'NN' in pos or 'JJ' in pos or 'VB' in pos:\n",
    "                if(not ign_words.get(h_parsed[2][word][0],0)):\n",
    "                    key_tokens.add(h_parsed[2][word][0])\n",
    "        self.lexicalGenerator.hypothesis_tokens = key_tokens\n",
    "        self.lexicalGenerator.hypothesis = self.hypothesis.replace(',','')\n",
    "        self.h_tree = h_tree\n",
    "        \n",
    "\n",
    "    def generate_premises(self, start):\n",
    "        self.entailments.clear()\n",
    "        self.contradictions.clear()\n",
    "\n",
    "        # Polarization from Udeo2Mono\n",
    "        start = self.phrasalGenerator.preprocess(start)\n",
    "        annotation = self.pipeline.single_polarization(start)\n",
    "\n",
    "        lexical_absolute = False\n",
    "\n",
    "        datapath = self.controller.controlPipeline(annotation['polarized_tree'], self.h_tree)\n",
    "        if len(datapath) == 1 and datapath[0][0] == 0:\n",
    "            lexical_absolute = True\n",
    "        print(\"Recommand: \", datapath)\n",
    "         \n",
    "        # Monotonicity-based Phrasal Inference\n",
    "        if not lexical_absolute:\n",
    "            self.phrasalGenerator.hypothesis = self.hypothesis.replace(',', '')\n",
    "\n",
    "            tokenized = tokenizer(start).sentences[0].words\n",
    "            tokens = [tok.text for tok in tokenized]\n",
    "\n",
    "            self.phrasalGenerator.deptree_generate(\n",
    "            annotation['polarized_tree'], \n",
    "            annotation['annotated'], tokens)\n",
    "            \n",
    "            if self.verbose == 10:\n",
    "                print(\"============================\")\n",
    "                print(\"Phrasal Inference\")\n",
    "\n",
    "            if self.phrasalGenerator.stop_critarion:\n",
    "                return True\n",
    "\n",
    "            for tree in self.phrasalGenerator.tree_log:\n",
    "                self.entailments.add((tree[1], tree[2]))\n",
    "            self.entailments |= set(self.phrasalGenerator.sent_log)\n",
    "\n",
    "            if self.verbose == 4 or self.verbose == 10:\n",
    "                print(*self.entailments, sep=\"\\n\")\n",
    "            \n",
    "\n",
    "            # Syntactic Vriation\n",
    "            # Sequence Chunking and Chunk Alignment from roBERTa\n",
    "            if self.verbose == 10:\n",
    "                print(\"Syntactic Vriation\")\n",
    "\n",
    "            sent_level = False\n",
    "            #print(self.current_optimal)\n",
    "            if self.current_optimal < 5.0:\n",
    "                sent_level = True\n",
    "            variates = self.syntacticVariator.variate(\n",
    "                start, \n",
    "                self.hypothesis, \n",
    "                annotation['polarized_tree'], \n",
    "                self.h_tree, sent_level)\n",
    "            for v in variates:\n",
    "                similarity = inference_sts(v, self.hypothesis, dist=True)\n",
    "                if self.verbose == 3 or self.verbose == 10:\n",
    "                    print(similarity, v)\n",
    "                if similarity < 0.5:\n",
    "                    return True\n",
    "                self.entailments.add((v, similarity))\n",
    "            if self.verbose == 2 or self.verbose == 10:\n",
    "                print(self.syntacticVariator.replacement_log)\n",
    "\n",
    "        # Monotonicity-based Lexical Inference\n",
    "        if self.verbose == 10:\n",
    "            print(\"Lexical Inference\")\n",
    "        \n",
    "        self.lexicalGenerator.premise = start\n",
    "        self.lexicalGenerator.hypothesis = self.hypothesis.replace(',', '')\n",
    "        if lexical_absolute:\n",
    "            #print(\"absoilute generation\")\n",
    "            self.lexicalGenerator.absolute_generate(datapath[0][3], datapath[0][4])\n",
    "            if self.lexicalGenerator.nondetermine:\n",
    "                return False\n",
    "\n",
    "        self.lexicalGenerator.deptree_generate(annotation['polarized_tree'])\n",
    "        \n",
    "        if self.verbose == 1 or self.verbose == 10:\n",
    "            print(self.lexicalGenerator.ant_ht)\n",
    "            print(self.lexicalGenerator.replacement_log)\n",
    "        \n",
    "        if self.lexicalGenerator.stop_critarion:\n",
    "            return True\n",
    "        for tree in self.lexicalGenerator.tree_log:\n",
    "            if self.verbose == 1 or self.verbose == 10:\n",
    "                print((tree[1], tree[2]))\n",
    "            self.entailments.add((tree[1], tree[2]))\n",
    "\n",
    "        return False\n",
    "\n",
    "    def generate(self, start, opened):\n",
    "        terminate = self.generate_premises(start)\n",
    "        if terminate:\n",
    "            return True\n",
    "\n",
    "        for premise in self.entailments:\n",
    "            if premise in self.closed:\n",
    "                continue\n",
    "            cost = premise[1]\n",
    "            if premise[0] not in opened:\n",
    "                opened[premise[0]] = cost\n",
    "            if cost < opened[premise[0]]:\n",
    "                opened[premise[0]] = cost\n",
    "        return False\n",
    "\n",
    "    def search(self, premises, hypothesis):\n",
    "        self.closed = pqdict({})\n",
    "        self.hypothesis = hypothesis\n",
    "        premises = premises.replace('\\n','')\n",
    "\n",
    "        self.hypothesis_kb()\n",
    "        self.phrasalGenerator.hypothesis = self.hypothesis\n",
    "        self.lexicalGenerator.hypothesis = self.hypothesis\n",
    "        self.lexicalGenerator.ant_ht = {}\n",
    "\n",
    "        open_lists = pqdict({})\n",
    "        open_lists[premises] = inference_sts(premises, hypothesis, dist=True)\n",
    "        self.current_optimal = open_lists[premises]\n",
    "\n",
    "        hop = 0\n",
    "        top_k = 1\n",
    "\n",
    "        while open_lists:\n",
    "            optimals = []\n",
    "            for _ in range(top_k):\n",
    "                if len(open_lists) > 0:\n",
    "                    optimals.append(open_lists.popitem())\n",
    "            \n",
    "            for optimal in optimals:\n",
    "                print(optimal[1], self.current_optimal)\n",
    "                if optimal[1] > self.current_optimal and (optimal[1]- self.current_optimal) > 3:\n",
    "                    return False\n",
    "                self.current_optimal = optimal[1]\n",
    "                self.closed[optimal] = len(self.closed) + 1\n",
    "                #if self.verbose == 7 or self.verbose == 10:\n",
    "                print(\"Optimal: \", optimal)\n",
    "                goal_found = self.generate(optimal[0], open_lists)\n",
    "                if goal_found:\n",
    "                    self.closed[(self.hypothesis, 0.0)] = len(self.closed) + 1\n",
    "                    return True   \n",
    "            hop += 1\n",
    "            if hop > 4:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Recommand:  [[0, 2, 2, 'men', 'people'], [0, 4, 4, 'carrying', 'walking'], [2, 8, 6, 'blankets', 'laden cc baskets'], [1, 5, -1, 'colorful', None], [2, 6, 6, 'baskets', 'laden cc baskets'], [2, 13, 6, 'building', 'laden cc baskets']]\n",
      "tensor(9.0657) tensor(14.3053)\n",
      "Optimal:  ('Two douth are carrying colorful baskets and blankets and walking near a building', tensor(9.0657))\n",
      "Recommand:  [[0, 2, 2, 'douth', 'people'], [0, 4, 4, 'carrying', 'walking'], [2, 8, 6, 'blankets', 'laden cc baskets'], [1, 5, -1, 'colorful', None], [2, 6, 6, 'baskets', 'laden cc baskets'], [2, 13, 6, 'building', 'laden cc baskets']]\n",
      " 84%| | 119/141 [33:14<04:22, 11.93s/it]tensor(7.1515) tensor(9.0657)\n",
      "Optimal:  ('Two douth are carrying colorful baskets and blankets walking', tensor(7.1515))\n",
      "tensor(9.4288) tensor(9.4288)\n",
      "Optimal:  ('Two people are carrying colorful baskets and blankets and walking near a building', tensor(9.4288))\n",
      "Recommand:  [[0, 4, 4, 'carrying', 'walking'], [2, 13, 6, 'building', 'laden cc baskets'], [2, 8, 6, 'blankets', 'laden cc baskets'], [1, 5, -1, 'colorful', None], [2, 6, 6, 'baskets', 'laden cc baskets']]\n",
      " 85%| | 120/141 [33:21<03:37, 10.36s/it]tensor(6.8693) tensor(9.4288)\n",
      "Optimal:  ('Two people are carrying colorful baskets and blankets walking', tensor(6.8693))\n",
      "tensor(5.2478) tensor(5.2478)\n",
      "Optimal:  ('Mud is covering a topless woman', tensor(5.2478))\n",
      "Recommand:  [[1, -1, 1, None, 'A'], [2, 1, 2, 'Mud', 'topless woman'], [1, -1, 5, None, 'being'], [2, 3, 6, 'covering', 'covered'], [2, 5, 8, 'topless woman', 'mud']]\n",
      " 86%| | 121/141 [33:22<02:30,  7.54s/it]tensor(9.0118) tensor(9.0118)\n",
      "Optimal:  ('An older man and a woman are riding bikes on the boardwalk near the water', tensor(9.0118))\n",
      "Recommand:  [[1, 5, -1, 'a', None], [1, -1, 2, None, 'middle'], [0, 1, 1, 'An', 'A'], [0, 2, 3, 'older', 'aged'], [1, -1, 15, None, 'water'], [1, 15, -1, 'water', None], [2, 12, 12, 'boardwalk', 'road']]\n",
      "tensor(8.0525) tensor(9.0118)\n",
      "Optimal:  ('A middle aged man and woman are riding bikes on the boardwalk near the water', tensor(8.0525))\n",
      "Recommand:  [[1, -1, 15, None, 'water'], [1, 15, -1, 'water', None], [2, 12, 12, 'boardwalk', 'road']]\n",
      "tensor(7.6046) tensor(8.0525)\n",
      "Optimal:  ('A middle aged man and woman are riding bikes on the boardwalks near the water', tensor(7.6046))\n",
      "Recommand:  [[1, -1, 15, None, 'water'], [1, 15, -1, 'water', None], [2, 12, 12, 'boardwalks', 'road']]\n",
      "tensor(7.2451) tensor(7.6046)\n",
      "Optimal:  ('A middle aged man and woman are riding bikes on the boardwalks near water', tensor(7.2451))\n",
      "Recommand:  [[1, -1, 15, None, 'water'], [1, 14, -1, 'water', None], [2, 12, 12, 'boardwalks', 'road']]\n",
      "tensor(7.1645) tensor(7.2451)\n",
      "Optimal:  ('an middle aged man and woman are riding bikes on the boardwalks near water', tensor(7.1645))\n",
      "Recommand:  [[0, 1, 1, 'an', 'A'], [1, -1, 15, None, 'water'], [1, 14, -1, 'water', None], [2, 12, 12, 'boardwalks', 'road']]\n",
      " 87%| | 123/141 [33:39<02:20,  7.80s/it]tensor(12.7865) tensor(12.7865)\n",
      "Optimal:  ('Pedestrians and cars are moving through a traffic jam in a big city', tensor(12.7865))\n",
      "Recommand:  [[1, -1, 2, None, 'people'], [0, 1, 4, 'Pedestrians', 'cars'], [1, 4, -1, 'are', None], [2, 5, 5, 'moving', 'are'], [1, -1, 8, None, 'crowded'], [2, 8, 9, 'traffic jam', 'street']]\n",
      "tensor(11.6889) tensor(12.7865)\n",
      "Optimal:  ('Pedestrians and cars are moving through a traffic jam in a crowd city', tensor(11.6889))\n",
      "Recommand:  [[1, -1, 2, None, 'people'], [0, 1, 4, 'Pedestrians', 'cars'], [1, 4, -1, 'are', None], [2, 5, 5, 'moving', 'are'], [1, 12, -1, 'crowd city', None], [1, -1, 8, None, 'crowded'], [2, 8, 9, 'traffic jam', 'street']]\n",
      " 88%| | 124/141 [34:15<04:40, 16.52s/it]tensor(13.2733) tensor(13.2733)\n",
      "Optimal:  ('Pedestrians and cars are moving through a traffic jam in a big city', tensor(13.2733))\n",
      "Recommand:  [[1, -1, 2, None, 'people'], [0, 1, 4, 'Pedestrians', 'vehicles'], [0, 3, 4, 'cars', 'vehicles'], [1, 4, -1, 'are', None], [2, 5, 5, 'moving', 'are'], [1, -1, 8, None, 'crowded'], [2, 8, 9, 'traffic jam', 'street']]\n",
      "tensor(12.5215) tensor(13.2733)\n",
      "Optimal:  ('Pedestrians and cars are moving through a traffic jam in a crowd city', tensor(12.5215))\n",
      "Recommand:  [[1, -1, 2, None, 'people'], [0, 3, 4, 'cars', 'vehicles'], [0, 1, 4, 'Pedestrians', 'vehicles'], [1, 4, -1, 'are', None], [2, 5, 5, 'moving', 'are'], [1, 12, -1, 'crowd city', None], [1, -1, 8, None, 'crowded'], [2, 8, 9, 'traffic jam', 'street']]\n",
      " 89%| | 125/141 [34:38<04:53, 18.31s/it]tensor(23.4738) tensor(23.4738)\n",
      "Optimal:  ('Two brown and white dogs are fighting on a grassy area in front of a tree', tensor(23.4738))\n",
      "Recommand:  [[1, 4, -1, 'white', None], [1, 2, -1, 'brown', None], [2, 5, 2, 'dogs', 'animals'], [1, -1, 5, None, 'outside'], [2, 7, 4, 'fighting', 'playing'], [1, 11, -1, 'area', None]]\n",
      "tensor(17.4863) tensor(23.4738)\n",
      "Optimal:  ('Two brown and white dogs are active on a grassy area in front of a tree', tensor(17.4863))\n",
      "Recommand:  [[1, 2, -1, 'brown', None], [1, 4, -1, 'white', None], [2, 5, 2, 'dogs', 'animals'], [1, -1, 5, None, 'outside'], [2, 7, 4, 'active', 'playing'], [1, 11, -1, 'area', None]]\n",
      "tensor(15.4492) tensor(17.4863)\n",
      "Optimal:  ('Two dogs are active on a grassy area in front of a tree', tensor(15.4492))\n",
      "Recommand:  [[0, 2, 2, 'dogs', 'animals'], [1, -1, 5, None, 'outside'], [2, 4, 4, 'active', 'playing'], [1, 8, -1, 'area', None]]\n",
      "tensor(13.3542) tensor(15.4492)\n",
      "Optimal:  ('Two dogs are active on a area in front of a tree', tensor(13.3542))\n",
      "Recommand:  [[0, 2, 2, 'dogs', 'animals'], [1, -1, 5, None, 'outside'], [2, 4, 4, 'active', 'playing'], [1, 7, -1, 'area', None]]\n",
      "tensor(12.3309) tensor(13.3542)\n",
      "Optimal:  ('Two dogs are active on a area', tensor(12.3309))\n",
      "Recommand:  [[0, 2, 2, 'dogs', 'animals'], [1, -1, 5, None, 'outside'], [2, 4, 4, 'active', 'playing'], [1, 7, -1, 'area', None]]\n",
      " 89%| | 126/141 [35:41<07:56, 31.74s/it]tensor(9.8145) tensor(9.8145)\n",
      "Optimal:  ('A toddler is making a splash inside a blue paddling pool', tensor(9.8145))\n",
      "Recommand:  [[2, 2, 2, 'toddler', 'child'], [0, 4, 4, 'making', 'splashing'], [1, -1, 5, None, 'water'], [1, 9, -1, 'blue', None], [2, 10, 8, 'paddling pool', 'pool'], [2, 6, 8, 'splash', 'pool']]\n",
      "tensor(7.4936) tensor(9.8145)\n",
      "Optimal:  ('A toddler is making a splash inside a water paddling pool', tensor(7.4936))\n",
      "Recommand:  [[2, 2, 2, 'toddler', 'child'], [0, 4, 4, 'making', 'splashing'], [1, -1, 5, None, 'water'], [2, 9, 8, 'water paddling pool', 'pool'], [2, 6, 8, 'splash', 'pool']]\n",
      "tensor(6.1806) tensor(7.4936)\n",
      "Optimal:  ('A toddler is making a splash inside a water toddle pool', tensor(6.1806))\n",
      "Recommand:  [[2, 2, 2, 'toddler', 'child'], [0, 4, 4, 'making', 'splashing'], [1, -1, 5, None, 'water'], [2, 9, 8, 'water toddle pool', 'pool'], [2, 6, 8, 'splash', 'pool']]\n",
      "tensor(5.8776) tensor(6.1806)\n",
      "Optimal:  ('A toddler is making a splash inside some water toddle pool', tensor(5.8776))\n",
      "Recommand:  [[2, 2, 2, 'toddler', 'child'], [0, 4, 4, 'making', 'splashing'], [1, -1, 5, None, 'water'], [2, 6, 8, 'splash', 'pool'], [2, 9, 8, 'water toddle pool', 'pool']]\n",
      "tensor(5.7970) tensor(5.8776)\n",
      "Optimal:  ('A child is making a splash inside some water toddle pool', tensor(5.7970))\n",
      "Recommand:  [[0, 1, 1, 'A', 'The'], [0, 4, 4, 'making', 'splashing'], [1, -1, 5, None, 'water'], [2, 9, 8, 'water toddle pool', 'pool'], [2, 6, 8, 'splash', 'pool']]\n",
      " 90%| | 127/141 [36:21<07:57, 34.11s/it]tensor(7.3759) tensor(7.3759)\n",
      "Optimal:  ('A man is standing on a black jeep on a dirt hill', tensor(7.3759))\n",
      "Recommand:  [[1, -1, 9, None, 'next'], [1, 7, -1, 'black', None], [2, 8, 7, 'jeep', 'dirt hill']]\n",
      " 91%| | 128/141 [36:22<05:16, 24.33s/it]tensor(14.3783) tensor(14.3783)\n",
      "Optimal:  ('A black dog and a small white and black dog are looking up at a kitchen countertop', tensor(14.3783))\n",
      "Recommand:  [[1, 2, -1, 'black', None], [2, 3, 2, 'dog', 'dogs'], [1, 7, -1, 'white', None], [1, 9, -1, 'black', None], [2, 10, 2, 'dog', 'dogs']]\n",
      " 91%|| 129/141 [36:35<04:11, 20.94s/it]tensor(12.3799) tensor(14.3783)\n",
      "Optimal:  ('A nigrify dog and a small white and black dog are looking up at a kitchen countertop', tensor(12.3799))\n",
      "tensor(8.4491) tensor(8.4491)\n",
      "Optimal:  ('A large dog and a small dog are standing next to the kitchen counter and are sniffing', tensor(8.4491))\n",
      "Recommand:  [[0, 17, 17, 'sniffing', 'investigating']]\n",
      "tensor(8.3567) tensor(8.4491)\n",
      "Optimal:  ('A large dog and a small dog are standing next to a kitchen counter and are sniffing', tensor(8.3567))\n",
      "Recommand:  [[0, 17, 17, 'sniffing', 'investigating'], [0, 12, 12, 'a', 'the']]\n",
      " 92%|| 130/141 [36:45<03:14, 17.68s/it]tensor(21.6215) tensor(21.6215)\n",
      "Optimal:  ('A dirty tan dog is rolling in the dirt and looking right at the camera', tensor(21.6215))\n",
      "Recommand:  [[1, 2, -1, 'dirty', None], [2, 3, 2, 'tan dog', 'dog'], [0, 6, 4, 'rolling', 'playing'], [2, 11, 4, 'looking', 'playing'], [2, 15, 7, 'camera', 'ground'], [2, 9, 7, 'dirt', 'ground']]\n",
      "tensor(18.9817) tensor(21.6215)\n",
      "Optimal:  ('A dirty tan dog is rolling in the dirt and looking right at the point and shooter', tensor(18.9817))\n",
      "Recommand:  [[1, 2, -1, 'dirty', None], [2, 3, 2, 'tan dog', 'dog'], [2, 11, 4, 'looking', 'playing'], [0, 6, 4, 'rolling', 'playing'], [2, 15, 7, 'point', 'ground'], [2, 17, 7, 'shooter', 'ground'], [2, 9, 7, 'dirt', 'ground']]\n",
      "tensor(17.5863) tensor(18.9817)\n",
      "Optimal:  ('A dirty tan dog is rolling in the dirt and front right at the point and shooter', tensor(17.5863))\n",
      "Recommand:  [[1, 2, -1, 'dirty', None], [2, 3, 2, 'tan dog', 'dog'], [0, 6, 4, 'rolling', 'playing'], [2, 11, 7, 'front', 'ground'], [2, 17, 7, 'shooter', 'ground'], [2, 9, 7, 'dirt', 'ground'], [2, 15, 7, 'point', 'ground']]\n",
      "tensor(16.4730) tensor(17.5863)\n",
      "Optimal:  ('A dirty tan dog is rolling in the ground and front right at the point and shooter', tensor(16.4730))\n",
      "Recommand:  [[1, 2, -1, 'dirty', None], [2, 3, 2, 'tan dog', 'dog'], [0, 6, 4, 'rolling', 'playing'], [2, 11, 7, 'front', 'ground'], [2, 15, 7, 'point', 'ground'], [0, 7, 5, 'in', 'on'], [2, 17, 7, 'shooter', 'ground']]\n",
      "tensor(15.5366) tensor(16.4730)\n",
      "Optimal:  ('A soil tan dog is rolling in the ground and front right at the point and shooter', tensor(15.5366))\n",
      "Recommand:  [[0, 2, 2, 'soil', 'dog'], [0, 3, 2, 'tan dog', 'dog'], [0, 6, 4, 'rolling', 'playing'], [2, 15, 7, 'point', 'ground'], [2, 11, 7, 'front', 'ground'], [0, 7, 5, 'in', 'on'], [2, 17, 7, 'shooter', 'ground']]\n",
      " 93%|| 131/141 [38:07<06:09, 36.95s/it]tensor(12.0649) tensor(12.0649)\n",
      "Optimal:  ('A person on a yellow dirt bike is taking a jump', tensor(12.0649))\n",
      "Recommand:  [[2, 2, 2, 'person', 'cyclist'], [1, 8, -1, 'is', None], [2, 9, 7, 'taking', 'is'], [1, 11, -1, 'jump', None], [1, -1, 8, None, 'airborne']]\n",
      " 94%|| 132/141 [38:09<03:56, 26.32s/it]tensor(7.5856) tensor(7.5856)\n",
      "Optimal:  ('People are clustered around a bonfire at night', tensor(7.5856))\n",
      "Recommand:  [[1, -1, 1, None, 'Several'], [0, 3, 4, 'clustered', 'gathered'], [1, -1, 9, None, 'night'], [2, 6, 7, 'bonfire', 'fire']]\n",
      " 94%|| 133/141 [38:10<02:30, 18.77s/it]tensor(20.9930) tensor(20.9930)\n",
      "Optimal:  ('A middle eastern man is standing with the back against a lamp post near to other people', tensor(20.9930))\n",
      "Recommand:  [[1, 3, -1, 'eastern', None], [1, 2, -1, 'middle', None], [1, -1, 4, None, 'leaning'], [0, 6, 10, 'standing', 'surrounded'], [2, 9, 7, 'back', 'pole'], [1, 17, -1, 'people', None], [2, 12, 7, 'lamp post', 'pole']]\n",
      "tensor(19.1412) tensor(20.9930)\n",
      "Optimal:  ('A middle eastern man is standing with the back against a post near to other people', tensor(19.1412))\n",
      "Recommand:  [[1, 3, -1, 'eastern', None], [1, 2, -1, 'middle', None], [1, -1, 4, None, 'leaning'], [0, 6, 10, 'standing', 'surrounded'], [2, 9, 7, 'back', 'pole']]\n",
      "tensor(17.5413) tensor(19.1412)\n",
      "Optimal:  ('A middle wait while man is standing with the back against a post near to other people', tensor(17.5413))\n",
      "Recommand:  [[1, -1, 2, None, 'man'], [1, 3, -1, 'wait', None], [1, -1, 10, None, 'surrounded'], [1, -1, 4, None, 'leaning'], [1, -1, 7, None, 'pole']]\n",
      "tensor(16.5409) tensor(17.5413)\n",
      "Optimal:  ('A middle wait while man is upright with the back against a post near to other people', tensor(16.5409))\n",
      "Recommand:  [[1, -1, 1, None, 'A'], [1, 7, -1, 'upright', None], [1, 3, -1, 'wait', None], [1, -1, 4, None, 'leaning'], [1, -1, 10, None, 'surrounded'], [2, 10, 7, 'back', 'pole'], [1, 17, -1, 'people', None], [2, 13, 7, 'post', 'pole']]\n",
      "tensor(15.9065) tensor(16.5409)\n",
      "Optimal:  ('A wait while man is upright with the back against a post near to other people', tensor(15.9065))\n",
      "Recommand:  [[1, -1, 2, None, 'man'], [1, 2, -1, 'wait', None], [1, -1, 4, None, 'leaning'], [1, -1, 10, None, 'surrounded'], [1, -1, 7, None, 'pole']]\n",
      " 95%|| 134/141 [39:25<04:09, 35.66s/it]tensor(18.5041) tensor(18.5041)\n",
      "Optimal:  ('A man and a woman are standing in front of an art gallery and are looking at a map', tensor(18.5041))\n",
      "Recommand:  [[2, 5, 2, 'woman', 'couple'], [0, 2, 2, 'man', 'couple'], [2, 7, 4, 'standing', 'looking'], [0, 15, 3, 'are', 'is'], [1, 12, -1, 'art gallery', None], [2, 9, 7, 'front', 'map']]\n",
      " 96%|| 135/141 [39:28<02:35, 25.89s/it]tensor(5.9926) tensor(5.9926)\n",
      "Optimal:  ('A large white dog is jumping up in the snow', tensor(5.9926))\n",
      "Recommand:  [[1, -1, 13, None, 'area'], [2, 10, 9, 'snow', 'air']]\n",
      "tensor(5.7064) tensor(5.9926)\n",
      "Optimal:  ('A large white dog is jumping up in an snow', tensor(5.7064))\n",
      "Recommand:  [[1, -1, 13, None, 'area'], [2, 10, 9, 'snow', 'air']]\n",
      "tensor(5.6089) tensor(5.7064)\n",
      "Optimal:  ('A large white dog is jumping up in some snow', tensor(5.6089))\n",
      "Recommand:  [[1, -1, 13, None, 'area'], [2, 10, 9, 'snow', 'air']]\n",
      "tensor(5.6089) tensor(5.6089)\n",
      "Optimal:  ('A large white dog is jumping up in some snow', tensor(5.6089))\n",
      "Recommand:  [[1, -1, 13, None, 'area'], [2, 10, 9, 'snow', 'air']]\n",
      "tensor(5.6089) tensor(5.6089)\n",
      "Optimal:  ('A large white dog is jumping up in some snow', tensor(5.6089))\n",
      "Recommand:  [[1, -1, 13, None, 'area'], [2, 10, 9, 'snow', 'air']]\n",
      " 96%|| 136/141 [39:34<01:38, 19.78s/it]tensor(18.0244) tensor(18.0244)\n",
      "Optimal:  ('A woman is wearing paint and costume pieces and is riding a bike on a empty street', tensor(18.0244))\n",
      "Recommand:  [[1, -1, 6, None, 'outfit'], [0, 2, 2, 'woman', 'girl'], [0, 4, 8, 'wearing', 'riding'], [2, 5, 10, 'paint', 'bike'], [0, 12, 9, 'a', 'the'], [2, 7, 10, 'costume pieces', 'bike'], [1, 16, -1, 'empty', None], [2, 17, 10, 'street', 'bike']]\n",
      "tensor(12.3462) tensor(18.0244)\n",
      "Optimal:  ('A woman is wearing paint and costume pieces is riding a bike', tensor(12.3462))\n",
      "Recommand:  [[1, -1, 6, None, 'outfit'], [0, 2, 2, 'woman', 'girl'], [0, 4, 8, 'wearing', 'riding'], [2, 5, 10, 'paint', 'bike'], [0, 11, 9, 'a', 'the'], [2, 7, 10, 'costume pieces', 'bike']]\n",
      "tensor(10.2187) tensor(12.3462)\n",
      "Optimal:  ('A woman is riding a bike', tensor(10.2187))\n",
      "Recommand:  [[1, -1, 6, None, 'outfit'], [0, 2, 2, 'woman', 'girl'], [0, 5, 9, 'a', 'the']]\n",
      "tensor(9.7659) tensor(10.2187)\n",
      "Optimal:  ('A girl is riding a bike', tensor(9.7659))\n",
      "Recommand:  [[1, -1, 6, None, 'outfit'], [0, 5, 9, 'a', 'the']]\n",
      "tensor(9.5454) tensor(9.7659)\n",
      "Optimal:  ('some girl is riding a bike', tensor(9.5454))\n",
      "Recommand:  [[1, -1, 6, None, 'outfit'], [0, 1, 1, 'some', 'A'], [0, 5, 9, 'a', 'the']]\n",
      " 97%|| 137/141 [40:18<01:49, 27.28s/it]tensor(15.5042) tensor(15.5042)\n",
      "Optimal:  ('A black dog and a yellow dog are playing with each other', tensor(15.5042))\n",
      "Recommand:  [[1, 2, -1, 'black', None], [2, 3, 2, 'dog', 'dogs'], [1, 6, -1, 'yellow', None], [2, 7, 2, 'dog', 'dogs']]\n",
      "tensor(12.0277) tensor(15.5042)\n",
      "Optimal:  ('A terr dog and a yellow dog are playing with each other', tensor(12.0277))\n",
      "Recommand:  [[1, 6, -1, 'yellow', None], [2, 7, 2, 'dog', 'dogs'], [1, 2, -1, 'terr', None], [2, 3, 2, 'dog', 'dogs']]\n",
      "tensor(10.2842) tensor(12.0277)\n",
      "Optimal:  ('A terr dog and a dog are playing with each other', tensor(10.2842))\n",
      "Recommand:  [[2, 2, 2, 'terr dog', 'dogs'], [2, 6, 2, 'dog', 'dogs']]\n",
      "tensor(9.2188) tensor(10.2842)\n",
      "Optimal:  ('terr dog a dog are playing with each other', tensor(9.2188))\n",
      "Recommand:  [[2, 4, 2, 'dog', 'dogs'], [1, 2, -1, 'dog', None]]\n",
      "tensor(8.8678) tensor(9.2188)\n",
      "Optimal:  ('dog a dog are playing with each other', tensor(8.8678))\n",
      "Recommand:  [[2, 1, 2, 'dog', 'dogs'], [2, 3, 2, 'dog', 'dogs']]\n",
      " 98%|| 138/141 [40:36<01:13, 24.40s/it]tensor(11.3555) tensor(11.3555)\n",
      "Optimal:  ('Two girl with dogs are walking on a forest path', tensor(11.3555))\n",
      "Recommand:  [[1, -1, 5, None, 'people'], [1, 4, -1, 'dogs', None], [2, 2, 2, 'girl', 'dogs'], [2, 9, 10, 'forest path', 'woods']]\n",
      "tensor(10.7554) tensor(11.3555)\n",
      "Optimal:  ('Two girl with dogs are walking in the woods', tensor(10.7554))\n",
      "Recommand:  [[1, -1, 5, None, 'people'], [1, 4, -1, 'dogs', None], [2, 2, 2, 'girl', 'dogs']]\n",
      "tensor(10.6355) tensor(10.7554)\n",
      "Optimal:  ('Two girl with dogs are walking in a woods', tensor(10.6355))\n",
      "Recommand:  [[1, -1, 5, None, 'people'], [1, 4, -1, 'dogs', None], [2, 2, 2, 'girl', 'dogs'], [0, 8, 9, 'a', 'the']]\n",
      "tensor(10.4831) tensor(10.6355)\n",
      "Optimal:  ('Two lady with dogs are walking in a woods', tensor(10.4831))\n",
      "Recommand:  [[1, -1, 2, None, 'dogs'], [1, 4, -1, 'dogs', None], [2, 2, 5, 'lady', 'people'], [0, 8, 9, 'a', 'the']]\n",
      "tensor(10.3304) tensor(10.4831)\n",
      "Optimal:  ('Two belady with dogs are walking in a woods', tensor(10.3304))\n",
      "Recommand:  [[1, -1, 2, None, 'dogs'], [1, 4, -1, 'dogs', None], [2, 2, 5, 'belady', 'people'], [0, 8, 9, 'a', 'the']]\n",
      " 99%|| 139/141 [40:53<00:44, 22.33s/it]tensor(16.6368) tensor(16.6368)\n",
      "Optimal:  ('A woman wearing a blue shirt and high heels is standing on the sidewalk next to a man', tensor(16.6368))\n",
      "Recommand:  [[1, -1, 5, None, 'woman'], [1, -1, 2, None, 'man'], [1, 2, -1, 'woman', None], [1, -1, 7, None, 'standing'], [1, -1, 10, None, 'curb']]\n",
      "tensor(9.7053) tensor(16.6368)\n",
      "Optimal:  ('A woman is standing on the sidewalk next to a man', tensor(9.7053))\n",
      "Recommand:  [[1, -1, 2, None, 'man'], [1, 8, -1, 'next', None], [0, 3, 6, 'is', 'are'], [0, 7, 10, 'sidewalk', 'curb']]\n",
      "tensor(7.3987) tensor(9.7053)\n",
      "Optimal:  ('A woman is standing on the curb next to a man', tensor(7.3987))\n",
      "Recommand:  [[1, -1, 2, None, 'man'], [1, 8, -1, 'next', None], [0, 3, 6, 'is', 'are']]\n",
      "tensor(7.3642) tensor(7.3987)\n",
      "Optimal:  ('A woman is standing on the curb next to an man', tensor(7.3642))\n",
      "Recommand:  [[1, -1, 2, None, 'man'], [1, 8, -1, 'next', None], [0, 3, 6, 'is', 'are']]\n",
      "tensor(7.3333) tensor(7.3642)\n",
      "Optimal:  ('A woman constitute standing on the curb next to an man', tensor(7.3333))\n",
      "Recommand:  [[1, -1, 2, None, 'man'], [1, -1, 6, None, 'are'], [2, 3, 7, 'constitute', 'standing'], [0, 4, 9, 'standing', 'the'], [1, 11, -1, 'man', None], [2, 8, 8, 'next', 'on']]\n",
      " 99%|| 140/141 [41:27<00:25, 25.71s/it]tensor(11.7762) tensor(11.7762)\n",
      "Optimal:  ('A black dog and a tan dog are fighting', tensor(11.7762))\n",
      "Recommand:  [[1, 2, -1, 'black', None], [2, 3, 2, 'dog', 'dogs'], [2, 6, 2, 'tan dog', 'dogs']]\n",
      "tensor(9.6987) tensor(11.7762)\n",
      "Optimal:  ('A mordant dog and a tan dog are fighting', tensor(9.6987))\n",
      "Recommand:  [[1, 2, -1, 'mordant', None], [2, 3, 2, 'dog', 'dogs'], [2, 6, 2, 'tan dog', 'dogs']]\n",
      "tensor(6.8716) tensor(9.6987)\n",
      "Optimal:  ('A mordant dog and a dog are fighting', tensor(6.8716))\n",
      "Recommand:  [[2, 6, 2, 'dog', 'dogs'], [1, 2, -1, 'mordant', None], [2, 3, 2, 'dog', 'dogs']]\n",
      "tensor(6.8458) tensor(6.8716)\n",
      "Optimal:  ('an mordant dog and a dog are fighting', tensor(6.8458))\n",
      "Recommand:  [[2, 6, 2, 'dog', 'dogs'], [1, 2, -1, 'mordant', None], [1, 1, -1, 'an', None], [2, 3, 2, 'dog', 'dogs']]\n",
      "tensor(6.8255) tensor(6.8458)\n",
      "Optimal:  ('an mordant dog and an dog are fighting', tensor(6.8255))\n",
      "Recommand:  [[1, 2, -1, 'mordant', None], [1, 1, -1, 'an', None], [1, -1, 1, None, 'Two'], [2, 3, 2, 'dog', 'dogs'], [1, 5, -1, 'an', None], [1, -1, 1, None, 'Two'], [2, 6, 2, 'dog', 'dogs']]\n",
      "100%|| 141/141 [41:44<00:00, 17.76s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "entail_p = []\n",
    "entail_hypo = []\n",
    "neutral_p = []\n",
    "neutral_hypo = []\n",
    "entail_imp_P = []\n",
    "entail_imp_hypo = []\n",
    "neutral_imp_P = []\n",
    "neutral_imp_hypo = []\n",
    "\n",
    "with open(\"../data/SICk/entail.txt\") as upward_med:\n",
    "    lines = upward_med.readlines()\n",
    "    for i in range(len(lines) // 3):\n",
    "        entail_p.append(lines[i*3])\n",
    "        entail_hypo.append(lines[i*3+1])\n",
    "\n",
    "with open(\"../data/SICk/neutral.txt\") as sick_noun:\n",
    "    lines = sick_noun.readlines()\n",
    "    for i in range(len(lines) // 3):\n",
    "        neutral_p.append(lines[i*3])\n",
    "        neutral_hypo.append(lines[i*3+1])\n",
    "\n",
    "with open(\"SICK_neutral_incorrect.txt\") as sick_noun:\n",
    "    lines = sick_noun.readlines()\n",
    "    for i in range(len(lines) // 4):\n",
    "        neutral_imp_P.append(lines[i*4+1])\n",
    "        neutral_imp_hypo.append(lines[i*4+2])\n",
    "\n",
    "with open(\"SICK_entail_incorrect.txt\") as sick_noun:\n",
    "    lines = sick_noun.readlines()\n",
    "    for i in range(len(lines) // 4):\n",
    "        entail_imp_P.append(lines[i*4+1])\n",
    "        entail_imp_hypo.append(lines[i*4+2])\n",
    "\n",
    "#print(len(entail_p))\n",
    "#2792\n",
    "planner = AStarPlanner(verbose=0)\n",
    "\n",
    "def evak_sick(premises, hypos):\n",
    "    with open(\"./entail_imp.txt\", 'w') as generate_log:\n",
    "        for i in tqdm(range(len(premises))):\n",
    "            premise = premises[i].replace('\\n', '').replace('Premise: ', \"\")\n",
    "            hypothesis = hypos[i].replace('\\n', '').replace('Hypothesis: ', \"\")\n",
    "            try:\n",
    "                entail = planner.search(premise, hypothesis)\n",
    "                if not entail:\n",
    "                    #print(\"\\nID: \" + str(i))\n",
    "                    generate_log.write(\"\\nID: \" + str(i))\n",
    "                    generate_log.write(\"\\nPremise: \" + premise)\n",
    "                    generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "                    generate_log.write('\\n')\n",
    "            except:\n",
    "                #continue\n",
    "                generate_log.write(\"\\nID: \" + str(i))\n",
    "                generate_log.write(\"\\nPremise: \" + premise)\n",
    "                generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "                generate_log.write('\\n')\n",
    "\n",
    "            #print(*planner.closed, sep=\" =>\\n\")\n",
    "\n",
    "evak_sick(entail_imp_P, entail_imp_hypo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(23.4738) tensor(23.4738)\n",
      "Optimal:  ('Two brown and white dogs are fighting on a grassy area in front of a tree', tensor(23.4738))\n",
      "Recommand:  [[1, 4, -1, 'white', None], [1, 2, -1, 'brown', None], [2, 5, 2, 'dogs', 'animals'], [1, -1, 5, None, 'outside'], [2, 7, 4, 'fighting', 'playing'], [1, 11, -1, 'area', None]]\n",
      "tensor(17.4863) tensor(23.4738)\n",
      "Optimal:  ('Two brown and white dogs are active on a grassy area in front of a tree', tensor(17.4863))\n",
      "Recommand:  [[1, 2, -1, 'brown', None], [1, 4, -1, 'white', None], [2, 5, 2, 'dogs', 'animals'], [1, -1, 5, None, 'outside'], [2, 7, 4, 'active', 'playing'], [1, 11, -1, 'area', None]]\n",
      "tensor(15.4492) tensor(17.4863)\n",
      "Optimal:  ('Two dogs are active on a grassy area in front of a tree', tensor(15.4492))\n",
      "Recommand:  [[0, 2, 2, 'dogs', 'animals'], [1, -1, 5, None, 'outside'], [2, 4, 4, 'active', 'playing'], [1, 8, -1, 'area', None]]\n",
      "tensor(13.3542) tensor(15.4492)\n",
      "Optimal:  ('Two dogs are active on a area in front of a tree', tensor(13.3542))\n",
      "Recommand:  [[0, 2, 2, 'dogs', 'animals'], [1, -1, 5, None, 'outside'], [2, 4, 4, 'active', 'playing'], [1, 7, -1, 'area', None]]\n",
      "tensor(12.3309) tensor(13.3542)\n",
      "Optimal:  ('Two dogs are active on a area', tensor(12.3309))\n",
      "Recommand:  [[0, 2, 2, 'dogs', 'animals'], [1, -1, 5, None, 'outside'], [2, 4, 4, 'active', 'playing'], [1, 7, -1, 'area', None]]\n",
      "('Two brown and white dogs are fighting on a grassy area in front of a tree', tensor(23.4738)) =>\n",
      "('Two brown and white dogs are active on a grassy area in front of a tree', tensor(17.4863)) =>\n",
      "('Two dogs are active on a grassy area in front of a tree', tensor(15.4492)) =>\n",
      "('Two dogs are active on a area in front of a tree', tensor(13.3542)) =>\n",
      "('Two dogs are active on a area', tensor(12.3309))\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "planner = AStarPlanner(verbose=7)\n",
    "entail = planner.search(entail_imp_P[125].replace('Premise: ', \"\"), entail_imp_hypo[125].replace('Hypothesis: ', \"\"))\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Optimal:  ('A little girl is playing a grand piano on stage', tensor(11.0662))\n",
      "Optimal:  ('A little girl is playing a imposing piano on stage', tensor(7.3023))\n",
      "Optimal:  ('A little girl is playing a tremendous piano on stage', tensor(7.4375))\n",
      "Optimal:  ('A little girl is playing a imposing piano on stage', tensor(6.5751))\n",
      "Optimal:  ('A little girl is playing a tremendous piano on stage', tensor(6.8940))\n",
      "Optimal:  ('A little girl is playing a imposing piano on stage', tensor(6.5751))\n",
      "Optimal:  ('A little girl is playing a tremendous piano on stage', tensor(6.8940))\n",
      "Optimal:  ('A little girl is playing a imposing piano on stage', tensor(6.5751))\n",
      "Optimal:  ('A little girl is playing a tremendous piano on stage', tensor(6.8940))\n",
      "Optimal:  ('A little girl is playing a imposing piano on stage', tensor(6.5751))\n",
      "Optimal:  ('A little girl is playing a tremendous piano on stage', tensor(6.8940))\n",
      "('A little girl is playing a grand piano on stage', tensor(11.0662)) =>\n",
      "('A little girl is playing a imposing piano on stage', tensor(7.3023)) =>\n",
      "('A little girl is playing a tremendous piano on stage', tensor(7.4375)) =>\n",
      "('A little girl is playing a imposing piano on stage', tensor(6.5751)) =>\n",
      "('A little girl is playing a tremendous piano on stage', tensor(6.8940)) =>\n",
      "('A little girl is playing a imposing piano on stage', tensor(6.5751)) =>\n",
      "('A little girl is playing a tremendous piano on stage', tensor(6.8940)) =>\n",
      "('A little girl is playing a imposing piano on stage', tensor(6.5751)) =>\n",
      "('A little girl is playing a tremendous piano on stage', tensor(6.8940)) =>\n",
      "('A little girl is playing a imposing piano on stage', tensor(6.5751)) =>\n",
      "('A little girl is playing a tremendous piano on stage', tensor(6.8940))\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "planner = AStarPlanner(verbose=7)\n",
    "entail = planner.search(entail_p[347], entail_hypo[347])\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(13.1200) tensor(13.1200)\n",
      "Optimal:  ('A few men are playing cricket', tensor(13.1200))\n",
      "Recommand:  [[0, 2, 2, 'men', 'people']]\n",
      "tensor(9.9960) tensor(13.1200)\n",
      "Optimal:  ('A-few person are playing cricket', tensor(9.9960))\n",
      "Recommand:  [[0, 2, 2, 'person', 'people']]\n",
      "tensor(5.0901) tensor(9.9960)\n",
      "Optimal:  ('a person are playing cricket', tensor(5.0901))\n",
      "Recommand:  [[2, 2, 2, 'person', 'people']]\n",
      "tensor(3.1770) tensor(5.0901)\n",
      "Optimal:  ('a people are playing cricket', tensor(3.1770))\n",
      "Recommand:  [[0, 1, 1, 'a', 'Some']]\n",
      "('A few men are playing cricket', tensor(13.1200)) =>\n",
      "('A-few person are playing cricket', tensor(9.9960)) =>\n",
      "('a person are playing cricket', tensor(5.0901)) =>\n",
      "('a people are playing cricket', tensor(3.1770)) =>\n",
      "('Some people are playing cricket', 0.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "planner = AStarPlanner(verbose=7)\n",
    "entail = planner.search(\"A few men are playing cricket\",\n",
    "        \"Some people are playing cricket\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================\n",
      "('Tambourines are being played by a group of children', tensor(0.9701)) =>\n",
      "('A group of children is playing tambourines', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"Tambourines are being played by a group of children\", \n",
    "             \"A group of children is playing tambourines\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A family is watching a little boy who is hitting a baseball', tensor(0.6364)) =>\n",
      "('a little boy who is hitting a baseball', tensor(0.8425)) =>\n",
      "('A child is watching a little boy who is hitting a baseball', tensor(0.7762)) =>\n",
      "('A child is hitting a baseball', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"A family is watching a little boy who is hitting a baseball\", \n",
    "             \"A child is hitting a baseball\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You know that some life changing actions must be taken when grandma reacts with the sad emoji', tensor(0.9259)) =>\n",
      "('You know that some life actions must be taken when grandma reacts with the sad emoji', tensor(0.9658)) =>\n",
      "('You know that some changing actions must be taken when grandma reacts with the sad emoji', tensor(0.9301)) =>\n",
      "('You know that some actions must be taken when grandma reacts with the sad emoji', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"You know that some life changing actions must be taken when grandma reacts with the sad emoji\", \"You know that some actions must be taken when grandma reacts with the sad emoji\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A black and a white dog are joyfully running on the grass', tensor(0.5195)) =>\n",
      "('A black and a white dog are running on the grass', tensor(0.9441)) =>\n",
      "('one black and a white dog are joyfully running on the grass', tensor(0.5451)) =>\n",
      "('A dog, which has a black coat, and a white dog are running on the grass', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"A black and a white dog are joyfully running on the grass\", \n",
    "             \"A dog, which has a black coat, and a white dog are running on the grass\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A group of boys are playing with a ball, in front of a large door made of wood', tensor(0.6323)) =>\n",
      "('A group of boys are playing in front of a large door made of wood', tensor(0.7649)) =>\n",
      "('A group of children are playing in front of a large door made of wood', tensor(0.8844)) =>\n",
      "('A group of boys are playing in front of a large door', tensor(0.8521)) =>\n",
      "('The children are playing in front of a large door', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"A group of boys are playing with a ball, in front of a large door made of wood\", \n",
    "             \"The children are playing in front of a large door\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A man with a red helmet is riding a blue motorcycle down the road', tensor(0.6541)) =>\n",
      "('A man is riding a blue motorcycle down the road', tensor(0.7365)) =>\n",
      "('A man is riding a motorbike down the road', tensor(0.9152)) =>\n",
      "('A man is is riding a motorbike a motorbike down the road', tensor(0.9281)) =>\n",
      "('A man is is riding motorbike a motorbike down the road', tensor(0.9314)) =>\n",
      "('man is is riding a motorbike a motorbike down the road', tensor(0.9309)) =>\n",
      "('A motorcyclist is riding a motorbike along a roadway', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"A man with a red helmet is riding a blue motorcycle down the road\", \n",
    "             \"A motorcyclist is riding a motorbike along a roadway\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal:  ('Two little boys are playing outside with a soccer ball on the green grass', tensor(0.8294))\n",
      "============================\n",
      "['are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on the green grass => with a soccer ball on the green grass', 'playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on the green grass => with a soccer ball on the green grass', 'on the green grass => on the green grass', 'a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on the green grass => with a soccer ball on the green grass']\n",
      "Two ['some', 'an', 'one']\n",
      "['Two => some', 'Two => an', 'Two => one', 'the => a', 'the => an', 'the => one', 'a => some', 'a => an', 'a => one']\n",
      "Optimal:  ('an little boys are playing outside with a soccer ball on the green grass', tensor(0.8907))\n",
      "============================\n",
      "['are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on the green grass => with a soccer ball on the green grass', 'playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on the green grass => with a soccer ball on the green grass', 'on the green grass => on the green grass', 'a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on the green grass => with a soccer ball on the green grass']\n",
      "['an => a', 'an => some', 'an => one', 'the => a', 'the => an', 'the => one', 'a => some', 'a => an', 'a => one']\n",
      "Optimal:  ('little boys are playing outside with a soccer ball on the green grass', tensor(0.8933))\n",
      "============================\n",
      "['are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on the green grass => with a soccer ball on the green grass', 'playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on the green grass => with a soccer ball on the green grass', 'on the green grass => on the green grass', 'a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on the green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on the green grass => with a soccer ball on the green grass']\n",
      "['the => a', 'the => an', 'the => one', 'a => some', 'a => an', 'a => one']\n",
      "Optimal:  ('little boys are playing outside with a soccer ball on green grass', tensor(0.8950))\n",
      "============================\n",
      "['are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on green grass => with a soccer ball on the green grass', 'a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on green grass => with a soccer ball on the green grass', 'playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on green grass => with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "['a => some', 'a => an', 'a => one']\n",
      "Optimal:  ('little boys are playing outside with soccer ball on green grass', tensor(0.8955))\n",
      "============================\n",
      "['soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with soccer ball on green grass => with a soccer ball on the green grass', 'are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with soccer ball on green grass => with a soccer ball on the green grass', 'playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "[]\n",
      "Optimal:  ('little boys are playing outside with an soccer ball on green grass', tensor(0.8944))\n",
      "============================\n",
      "['playing outside with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with an soccer ball on green grass => with a soccer ball on the green grass', 'are playing outside with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with an soccer ball on green grass => with a soccer ball on the green grass', 'an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with an soccer ball on green grass => with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "['an => a', 'an => some', 'an => one']\n",
      "Optimal:  ('little boys are playing outside with soccer ball on green grass', tensor(0.8955))\n",
      "============================\n",
      "['soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with soccer ball on green grass => with a soccer ball on the green grass', 'are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with soccer ball on green grass => with a soccer ball on the green grass', 'playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "[]\n",
      "Optimal:  ('little boys are playing outside with a soccer ball on green grass', tensor(0.8950))\n",
      "============================\n",
      "['are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on green grass => with a soccer ball on the green grass', 'a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on green grass => with a soccer ball on the green grass', 'playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on green grass => with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "['a => some', 'a => an', 'a => one']\n",
      "Optimal:  ('little boys are playing outside with soccer ball on green grass', tensor(0.8955))\n",
      "============================\n",
      "['soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with soccer ball on green grass => with a soccer ball on the green grass', 'are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with soccer ball on green grass => with a soccer ball on the green grass', 'playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "[]\n",
      "Optimal:  ('little boys are playing outside with an soccer ball on green grass', tensor(0.8944))\n",
      "============================\n",
      "['playing outside with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with an soccer ball on green grass => with a soccer ball on the green grass', 'are playing outside with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with an soccer ball on green grass => with a soccer ball on the green grass', 'an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with an soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with an soccer ball on green grass => with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "['an => a', 'an => some', 'an => one']\n",
      "Optimal:  ('little boys are playing outside with soccer ball on green grass', tensor(0.8955))\n",
      "============================\n",
      "['soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with soccer ball on green grass => with a soccer ball on the green grass', 'are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with soccer ball on green grass => with a soccer ball on the green grass', 'playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "[]\n",
      "Optimal:  ('little boys are playing outside with a soccer ball on green grass', tensor(0.8950))\n",
      "============================\n",
      "['are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'Somebody are playing outside with a soccer ball on green grass => with a soccer ball on the green grass', 'a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'with a soccer ball on green grass => with a soccer ball on the green grass', 'playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on green grass => are playing outside with a soccer ball on the green grass', 'are playing outside with a soccer ball on green grass => with a soccer ball on the green grass', 'on green grass => on the green grass']\n",
      "['a => some', 'a => an', 'a => one']\n",
      "('Two little boys are playing outside with a soccer ball on the green grass', tensor(0.8294)) =>\n",
      "('an little boys are playing outside with a soccer ball on the green grass', tensor(0.8907)) =>\n",
      "('little boys are playing outside with a soccer ball on the green grass', tensor(0.8933)) =>\n",
      "('little boys are playing outside with a soccer ball on green grass', tensor(0.8950)) =>\n",
      "('little boys are playing outside with soccer ball on green grass', tensor(0.8955)) =>\n",
      "('little boys are playing outside with an soccer ball on green grass', tensor(0.8944)) =>\n",
      "('little boys are playing outside with soccer ball on green grass', tensor(0.8955)) =>\n",
      "('little boys are playing outside with a soccer ball on green grass', tensor(0.8950)) =>\n",
      "('little boys are playing outside with soccer ball on green grass', tensor(0.8955)) =>\n",
      "('little boys are playing outside with an soccer ball on green grass', tensor(0.8944)) =>\n",
      "('little boys are playing outside with soccer ball on green grass', tensor(0.8955)) =>\n",
      "('little boys are playing outside with a soccer ball on green grass', tensor(0.8950))\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(entail_p[52], entail_hypo[52])\n",
    "#entail = planner.search(\"A dog that has a black and white coat is trotting through shallow water\", \n",
    "#             \"A dog that has a white and black colored coat is trotting through shallow water.\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal:  ('The leading car gradually shifted to the left lane', tensor(0.9242))\n",
      "============================\n",
      "('The leading car gradually shifted to the left lane', tensor(0.9242)) =>\n",
      "('The leading car slowly shifted to the left lane', 1.0)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "entail = planner.search(\"The leading car gradually shifted to the left lane\", \n",
    "             \"The leading car slowly shifted to the left lane\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entail = planner.search(\"A family is watching a little boy who is hitting a baseball\", \n",
    "             \"A child is hitting a baseball\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entail = planner.search(\"A deer is jumping over a fence\", \n",
    "             \"A deer is jumping a fence\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entail = planner.search(\"A boy is hitting a baseball\", \n",
    "             \"A chil is hitting a baseball\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entail = planner.search(\"A brown dog is attacking another animal in front of the tall man in pants\", \n",
    "             \"A dog is attacking another animal in front of the man in pants\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entail = planner.search(\"A family is watching a little boy who is hitting a baseball\", \n",
    "             \"A family is watching a boy who is hitting a baseball\")\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entail = planner.search(\"You can't park in front of my house on weekends.\", \n",
    "             \"You can't park in front of my large house on weekends.\")\n",
    "\n",
    "print(*planner.closed, sep=\" =>\\n\")\n",
    "print(entail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "annotations = []\n",
    "with open(\"./generation_log_upward.txt\", 'w') as generate_log:\n",
    "    phrasalGenerator = PhrasalGenerator()\n",
    "    pipeline = PolarizationPipeline(verbose=0)\n",
    "    for i in tqdm(range(0, 500)):\n",
    "        premise = MED_none[i].replace('\\n', '')\n",
    "        hypothesis = MED_none_hypo[i].replace('\\n', '')\n",
    "        premise = phrasalGenerator.preprocess(premise)\n",
    "        hypothesis = phrasalGenerator.preprocess(hypothesis)\n",
    "\n",
    "        tokenized = tokenizer(premise).sentences[0].words\n",
    "        tokens = [tok.text for tok in tokenized]\n",
    "\n",
    "        try:\n",
    "            h_parsed, replaced = dependency_parse(hypothesis, parser=\"stanza\")\n",
    "            h_tree, _ = pipeline.run_binarization(h_parsed, hypothesis, {})\n",
    "        except:\n",
    "            generate_log.write(\"\\nPremise: \" + premise)\n",
    "            generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "            continue\n",
    "        pipeline.modify_replacement(h_tree, replaced)\n",
    "        phrases = {} \n",
    "        collect_modifiers(h_tree, phrases, mod_type=\"NN\")\n",
    "        collect_modifiers(h_tree, phrases, mod_type=\"VB\")\n",
    "\n",
    "        try:\n",
    "            annotation = pipeline.single_polarization(premise)\n",
    "        except:\n",
    "            #generate_log.write(\"\\nPremise: \" + premise)\n",
    "            #generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "            continue\n",
    "    \n",
    "        phrasalGenerator.kb = phrases\n",
    "        #print(phrasalGenerator.kb)\n",
    "        phrasalGenerator.hypothesis = hypothesis.replace(',', '')\n",
    "        \n",
    "        phrasalGenerator.deptree_generate(\n",
    "            annotation['polarized_tree'], \n",
    "            annotation['annotated'], \n",
    "            tokens)\n",
    "\n",
    "        # for gen_tree in phrasalGenerator.tree_log:\n",
    "        #    leaves = gen_tree[0].sorted_leaves().popkeys()\n",
    "        #    sentence = ' '.join([x[0] for x in leaves])\n",
    "        #    print((sentence, gen_tree[1]))\n",
    "            \n",
    "        if phrasalGenerator.stop_critarion:\n",
    "            generate_log.write(\"\\nID: \" + str(i))\n",
    "            generate_log.write(\"\\nPremise: \" + premise)\n",
    "            generate_log.write(\"\\nHypothesis: \" + hypothesis)\n",
    "            #print(\"\\nPremise: \" + premise)\n",
    "            #print(\"\\nHypothesis: \" + hypothesis)\n",
    "            #print(*phrasalGenerator.sent_log, sep=\"\\n\")\n",
    "            #generate_log.writelines(phrasalGenerator.sent_log)\n",
    "            generate_log.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "entail_p = []\n",
    "entail_hypo = []\n",
    "\n",
    "with open(\"generation_log_SICK_Neutral.txt\", 'r') as upward_med:\n",
    "    lines = upward_med.readlines()\n",
    "    for i in range(len(lines) // 4):\n",
    "        entail_p.append(lines[i*4+1])\n",
    "        entail_hypo.append(lines[i*4+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"SICK_neutral_incorrect.txt\", 'w') as generate_log:\n",
    "    for i in range(len(entail_p)):\n",
    "        generate_log.write(\"ID: \" + str(i) + '\\n')\n",
    "        generate_log.write(entail_p[i])\n",
    "        generate_log.write(entail_hypo[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "confusion1 = np.asarray([[435, 0,   979], \n",
    "                         [0,   598, 122], \n",
    "                         [136, 55,   2606]])\n",
    "\n",
    "confusion2 = np.asarray([[1026, 0,   388], \n",
    "                         [0,   598,  122], \n",
    "                         [840, 55,   1902]])\n",
    "\n",
    "confusion_full = np.asarray([[1272, 0,   142], \n",
    "                             [0,   598,  122], \n",
    "                             [142, 55,   2600]])\n",
    "\n",
    "def precision(conf):\n",
    "    p = []\n",
    "    for i in range(3):\n",
    "        pre = conf[i][i] / (conf[i][0] + conf[i][1] + conf[i][2])\n",
    "        p.append(round(pre, 4))\n",
    "    return p\n",
    "\n",
    "def avg(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6899666666666665"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "avg(precision(confusion1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7935333333333333"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "avg(precision(confusion1.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7454000000000001"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "avg(precision(confusion2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.7514"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "avg(precision(confusion2.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8865999999999999"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "avg(precision(confusion_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9077333333333333"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "avg(precision(confusion_full.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}